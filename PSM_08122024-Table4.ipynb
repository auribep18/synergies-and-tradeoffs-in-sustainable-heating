{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cf2625b-146a-4196-a11d-4c3a135d2af7",
   "metadata": {},
   "source": [
    "##  “Are there synergies and tradeoffs in sustainable heating from cleaner stoves and home insulation? Evidence from air pollution control policies in southern Chile” (Ref.: ENEECO-D-24-01813)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb9849-289f-49a2-bd76-4702001f46f0",
   "metadata": {},
   "source": [
    "###  Table 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a3c005-4ab9-4480-9ce9-dc96f0219fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from linearmodels.panel import PanelOLS\n",
    "# Set datetime string\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import pairwise_distances\n",
    "datetime_string = datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ea075-1e76-40f2-a58e-bc29773a19c3",
   "metadata": {},
   "source": [
    "#### Function to extract results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29192180-0ce9-454a-a880-8657f8d23847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract T1ON results\n",
    "def extract_t1on_results(results, model_name):\n",
    "    \"\"\"\n",
    "    Extract T1ON parameter, standard error, t-stat, p-value, and confidence intervals from PanelOLS results.\n",
    "    \"\"\"\n",
    "    param = results.params[\"T1ON\"]\n",
    "    std_err = results.std_errors[\"T1ON\"]\n",
    "    t_stat = results.tstats[\"T1ON\"]\n",
    "    p_value = results.pvalues[\"T1ON\"]\n",
    "    ci_lower = param - 1.96 * std_err\n",
    "    ci_upper = param + 1.96 * std_err\n",
    "\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"PELLET*ON Coefficient\": param,\n",
    "        \"Std. Error\": std_err,\n",
    "        \"T-stat\": t_stat,\n",
    "        \"P-value\": p_value,\n",
    "        \"CI Lower\": ci_lower,\n",
    "        \"CI Upper\": ci_upper,\n",
    "    }\n",
    "\n",
    "# Initialize a DataFrame to store results\n",
    "t1on_results_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc62f38-29f4-4842-8dad-cb2f467264e8",
   "metadata": {},
   "source": [
    " #### Table 4. column 1.  The Effect of Pellet Stoves on PM2.5 Concentration (hourly observations)  - without matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1392c953-4f86-4d77-a2c9-d4cc7ff0e72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:                log_PMi   R-squared:                        0.4847\n",
      "Estimator:                   PanelOLS   R-squared (Between):              0.7651\n",
      "No. Observations:               17687   R-squared (Within):               0.4847\n",
      "Date:                Sun, Dec 08 2024   R-squared (Overall):              0.7625\n",
      "Time:                        02:04:23   Log-likelihood                   -2770.3\n",
      "Cov. Estimator:             Clustered                                           \n",
      "                                        F-statistic:                      2712.9\n",
      "Entities:                         379   P-value                           0.0000\n",
      "Avg Obs:                       46.668   Distribution:                 F(6,17302)\n",
      "Min Obs:                       7.0000                                           \n",
      "Max Obs:                       49.000   F-statistic (robust):             173.08\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                     865   Distribution:                 F(6,17302)\n",
      "Avg Obs:                       20.447                                           \n",
      "Min Obs:                       1.0000                                           \n",
      "Max Obs:                       35.000                                           \n",
      "                                                                                \n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "ON             0.0706     0.0179     3.9498     0.0001      0.0355      0.1056\n",
      "T1ON          -0.1275     0.0221    -5.7607     0.0000     -0.1709     -0.0841\n",
      "log_PMo        0.5770     0.0201     28.643     0.0000      0.5376      0.6165\n",
      "log_To        -0.0084     0.0130    -0.6496     0.5160     -0.0338      0.0170\n",
      "DayExp        -0.0186     0.0076    -2.4366     0.0148     -0.0335     -0.0036\n",
      "Cycle         -0.0021     0.0045    -0.4729     0.6363     -0.0110      0.0067\n",
      "==============================================================================\n",
      "\n",
      "F-test for Poolability: 21.094\n",
      "P-value: 0.0000\n",
      "Distribution: F(378,17302)\n",
      "\n",
      "Included effects: Entity\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "relevant_vars = [\"ID\",\"IDHour\",\"T1ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\",\"ON\", \"log_PMi\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[relevant_vars]\n",
    "df= df.dropna(subset=[\"log_PMi\",\"ON\", \"T1ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"])\n",
    "# Save merged dataset\n",
    "panel_data_path = f\"PanelBDTemuco_{datetime_string}.csv\"\n",
    "df.to_csv(panel_data_path, index=False)\n",
    "# Prepare for panel regression\n",
    "df = df.sort_values(by=[\"ID\", \"IDHour\"])\n",
    "df = df.set_index([\"ID\", \"IDHour\"])\n",
    "\n",
    "from linearmodels.panel import PanelOLS\n",
    "# Define independent variables and dependent variable\n",
    "dependent_var = \"log_PMi\"\n",
    "independent_vars = [\"ON\", \"T1ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"]\n",
    "# Fixed Effects Model\n",
    "model = PanelOLS.from_formula(\n",
    "    #f\"{dependent_var} ~ { ' + '.join(independent_vars)} + EntityEffects + TimeEffects\",\n",
    "    f\"{dependent_var} ~ { ' + '.join(independent_vars)} + EntityEffects\",\n",
    "    data=df,\n",
    "    drop_absorbed=True\n",
    ")\n",
    "results = model.fit(cov_type=\"clustered\", cluster_entity=True)\n",
    "# Extract T1ON results for the current model\n",
    "t1on_result = extract_t1on_results(results, model_name=\"model 1 - Without Matching\")\n",
    "t1on_results_df = pd.concat([t1on_results_df, pd.DataFrame([t1on_result])], ignore_index=True)\n",
    "print(results.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f47243-5d47-45d7-a2d4-470e7650f676",
   "metadata": {},
   "source": [
    " #### Table 4. column 1.  The Effect of Pellet Stoves on PM2.5 Concentration (hourly observations)  - with matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8515ab39-4e26-4bd9-ae2d-84bea3f8b862",
   "metadata": {},
   "source": [
    "#### Perform nearest neighbor matching (neighbors = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9aace60-221d-4718-83bd-fad4d89424a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.660393\n",
      "         Iterations 5\n",
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:                log_PMi   R-squared:                        0.5098\n",
      "Estimator:                   PanelOLS   R-squared (Between):              0.7820\n",
      "No. Observations:               17687   R-squared (Within):               0.5098\n",
      "Date:                Sun, Dec 08 2024   R-squared (Overall):              0.7792\n",
      "Time:                        02:04:23   Log-likelihood                   -2129.3\n",
      "Cov. Estimator:             Clustered                                           \n",
      "                                        F-statistic:                      2999.4\n",
      "Entities:                         379   P-value                           0.0000\n",
      "Avg Obs:                       46.668   Distribution:                 F(6,17302)\n",
      "Min Obs:                       7.0000                                           \n",
      "Max Obs:                       49.000   F-statistic (robust):             158.28\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                     865   Distribution:                 F(6,17302)\n",
      "Avg Obs:                       20.447                                           \n",
      "Min Obs:                       1.0000                                           \n",
      "Max Obs:                       35.000                                           \n",
      "                                                                                \n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "ON             0.0403     0.0203     1.9834     0.0473      0.0005      0.0802\n",
      "T1ON          -0.0925     0.0244    -3.7985     0.0001     -0.1403     -0.0448\n",
      "log_PMo        0.5957     0.0220     27.088     0.0000      0.5526      0.6388\n",
      "log_To        -0.0126     0.0143    -0.8812     0.3782     -0.0406      0.0154\n",
      "DayExp        -0.0183     0.0088    -2.0937     0.0363     -0.0355     -0.0012\n",
      "Cycle         -0.0072     0.0052    -1.4050     0.1600     -0.0174      0.0029\n",
      "==============================================================================\n",
      "\n",
      "F-test for Poolability: 21.123\n",
      "P-value: 0.0000\n",
      "Distribution: F(378,17302)\n",
      "\n",
      "Included effects: Entity\n"
     ]
    }
   ],
   "source": [
    "# Perform nearest neighbor matching\n",
    "neighbors = 2\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "# Filter data\n",
    "df = df[df['tper'] < 2]\n",
    "# Define the variables to keep\n",
    "variables_to_keep = [\"ID\",\"T1\",\"Education\", \"GenderHHead1women\", \"Age\", \"Disability\", \"NSize\", \"Anypersonolder60\", \"Chamber\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[variables_to_keep]\n",
    "\n",
    "# Impute missing values with the median of each column\n",
    "columns_to_impute = [\"Education\", \"GenderHHead1women\", \"Age\"]\n",
    "for col in columns_to_impute:\n",
    "    median_value = df[col].median()  # Calculate the median\n",
    "    df[col] = df[col].fillna(median_value)  # Replace missing values with the median\n",
    "# Logistic regression to estimate propensity scores\n",
    "covariates = [\"Education\", \"GenderHHead1women\", \"Age\", \"Disability\", \"NSize\", \"Anypersonolder60\", \"Chamber\"]\n",
    "X = sm.add_constant(df[covariates])\n",
    "y = df[\"T1\"]\n",
    "logit_model = sm.Logit(y, X).fit()\n",
    "df[\"logoddsT1\"] = logit_model.predict(X)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "logit_model = LogisticRegression(max_iter=500)\n",
    "df[\"propensity_score\"] = logit_model.fit(X_scaled, y).predict_proba(X_scaled)[:, 1]\n",
    "# Separate treated and control groups\n",
    "treated = df[df[\"T1\"] == 1]\n",
    "control = df[df[\"T1\"] == 0]\n",
    "\n",
    "# Apply caliper (0.01)\n",
    "caliper = 0.01\n",
    "matches4 = []\n",
    "nn = NearestNeighbors(n_neighbors=neighbors, metric=\"euclidean\")\n",
    "nn.fit(control[[\"propensity_score\"]])\n",
    "distances, indices = nn.kneighbors(treated[[\"propensity_score\"]])\n",
    "matches4 = []\n",
    "for i, dists in enumerate(distances):\n",
    "    matched_indices = indices[i][dists <= caliper]\n",
    "    if len(matched_indices) > 0:\n",
    "        for control_index in matched_indices:\n",
    "            matches4.append((treated.index[i], control.iloc[control_index].name))\n",
    "# Generate matched DataFrame\n",
    "matched_pairs = pd.DataFrame(matches4, columns=[\"treated_index\", \"control_index\"])\n",
    "# Assign weights\n",
    "df[\"_weight\"] = 0.0\n",
    "df.loc[treated.index, \"_weight\"] = 1.0\n",
    "for treated_idx in matched_pairs[\"treated_index\"].unique():\n",
    "    controls = matched_pairs[matched_pairs[\"treated_index\"] == treated_idx][\"control_index\"]\n",
    "    df.loc[controls, \"_weight\"] += 1 / len(controls)\n",
    "df[\"weight\"] = df[\"_weight\"]\n",
    "# Replace zero weights with a small positive value\n",
    "df[\"weight\"] = df[\"weight\"].apply(lambda x: x if x > 0 else 1e-08)\n",
    "#df[\"weight\"] = df[\"weight\"] / df[\"weight\"].mean()\n",
    "# Save weights to file\n",
    "weights_path = f\"Weights_{datetime_string}.csv\"\n",
    "df[[\"ID\", \"weight\"]].to_csv(weights_path, index=False)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "relevant_vars = [\"ID\",\"IDHour\",\"T1ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\",\"ON\", \"log_PMi\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[relevant_vars]\n",
    "df= df.dropna(subset=[\"log_PMi\",\"ON\", \"T1ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"])\n",
    "# Merge weights into original data\n",
    "weights_df = pd.read_csv(weights_path)\n",
    "df = pd.merge(df, weights_df, on=\"ID\", how=\"left\")\n",
    "# Save merged dataset\n",
    "panel_data_path = f\"PanelBDTemuco_{datetime_string}.csv\"\n",
    "df.to_csv(panel_data_path, index=False)\n",
    "# Prepare for panel regression\n",
    "df = df.sort_values(by=[\"ID\", \"IDHour\"])\n",
    "df = df.set_index([\"ID\", \"IDHour\"])\n",
    "\n",
    "model = PanelOLS.from_formula(\n",
    "    f\"{dependent_var} ~ { ' + '.join(independent_vars)} + EntityEffects\",\n",
    "    data=df,\n",
    "    weights=df[\"weight\"],\n",
    "    drop_absorbed=True\n",
    ")\n",
    "results = model.fit(cov_type=\"clustered\", cluster_entity=True)\n",
    "# Extract T1ON results for the current model\n",
    "t1on_result = extract_t1on_results(results, model_name=\"model 2, Nearest Neighbors Matching, n = 2\")\n",
    "t1on_results_df = pd.concat([t1on_results_df, pd.DataFrame([t1on_result])], ignore_index=True)\n",
    "print(results.summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0efe1d66-2613-45a6-95e6-a51f887b37a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.660393\n",
      "         Iterations 5\n",
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:                log_PMi   R-squared:                        0.4919\n",
      "Estimator:                   PanelOLS   R-squared (Between):              0.7653\n",
      "No. Observations:               17687   R-squared (Within):               0.4919\n",
      "Date:                Sun, Dec 08 2024   R-squared (Overall):              0.7624\n",
      "Time:                        02:04:23   Log-likelihood                   -2440.6\n",
      "Cov. Estimator:             Clustered                                           \n",
      "                                        F-statistic:                      2792.1\n",
      "Entities:                         379   P-value                           0.0000\n",
      "Avg Obs:                       46.668   Distribution:                 F(6,17302)\n",
      "Min Obs:                       7.0000                                           \n",
      "Max Obs:                       49.000   F-statistic (robust):             157.28\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                     865   Distribution:                 F(6,17302)\n",
      "Avg Obs:                       20.447                                           \n",
      "Min Obs:                       1.0000                                           \n",
      "Max Obs:                       35.000                                           \n",
      "                                                                                \n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "ON             0.0447     0.0213     2.0983     0.0359      0.0029      0.0865\n",
      "T1ON          -0.0972     0.0257    -3.7784     0.0002     -0.1476     -0.0468\n",
      "log_PMo        0.5818     0.0222     26.192     0.0000      0.5383      0.6254\n",
      "log_To        -0.0243     0.0151    -1.6087     0.1077     -0.0539      0.0053\n",
      "DayExp        -0.0182     0.0084    -2.1527     0.0314     -0.0347     -0.0016\n",
      "Cycle         -0.0022     0.0051    -0.4235     0.6719     -0.0122      0.0079\n",
      "==============================================================================\n",
      "\n",
      "F-test for Poolability: 20.640\n",
      "P-value: 0.0000\n",
      "Distribution: F(378,17302)\n",
      "\n",
      "Included effects: Entity\n"
     ]
    }
   ],
   "source": [
    "# Perform nearest neighbor matching\n",
    "neighbors = 3\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "# Filter data\n",
    "df = df[df['tper'] < 2]\n",
    "# Define the variables to keep\n",
    "variables_to_keep = [\"ID\",\"T1\",\"Education\", \"GenderHHead1women\", \"Age\", \"Disability\", \"NSize\", \"Anypersonolder60\", \"Chamber\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[variables_to_keep]\n",
    "\n",
    "# Impute missing values with the median of each column\n",
    "columns_to_impute = [\"Education\", \"GenderHHead1women\", \"Age\"]\n",
    "for col in columns_to_impute:\n",
    "    median_value = df[col].median()  # Calculate the median\n",
    "    df[col] = df[col].fillna(median_value)  # Replace missing values with the median\n",
    "# Logistic regression to estimate propensity scores\n",
    "covariates = [\"Education\", \"GenderHHead1women\", \"Age\", \"Disability\", \"NSize\", \"Anypersonolder60\", \"Chamber\"]\n",
    "X = sm.add_constant(df[covariates])\n",
    "y = df[\"T1\"]\n",
    "logit_model = sm.Logit(y, X).fit()\n",
    "df[\"logoddsT1\"] = logit_model.predict(X)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "logit_model = LogisticRegression(max_iter=500)\n",
    "df[\"propensity_score\"] = logit_model.fit(X_scaled, y).predict_proba(X_scaled)[:, 1]\n",
    "# Separate treated and control groups\n",
    "treated = df[df[\"T1\"] == 1]\n",
    "control = df[df[\"T1\"] == 0]\n",
    "\n",
    "# Apply caliper (0.01)\n",
    "caliper = 0.01\n",
    "matches4 = []\n",
    "nn = NearestNeighbors(n_neighbors=neighbors, metric=\"euclidean\")\n",
    "nn.fit(control[[\"propensity_score\"]])\n",
    "distances, indices = nn.kneighbors(treated[[\"propensity_score\"]])\n",
    "matches4 = []\n",
    "for i, dists in enumerate(distances):\n",
    "    matched_indices = indices[i][dists <= caliper]\n",
    "    if len(matched_indices) > 0:\n",
    "        for control_index in matched_indices:\n",
    "            matches4.append((treated.index[i], control.iloc[control_index].name))\n",
    "# Generate matched DataFrame\n",
    "matched_pairs = pd.DataFrame(matches4, columns=[\"treated_index\", \"control_index\"])\n",
    "# Assign weights\n",
    "df[\"_weight\"] = 0.0\n",
    "df.loc[treated.index, \"_weight\"] = 1.0\n",
    "for treated_idx in matched_pairs[\"treated_index\"].unique():\n",
    "    controls = matched_pairs[matched_pairs[\"treated_index\"] == treated_idx][\"control_index\"]\n",
    "    df.loc[controls, \"_weight\"] += 1 / len(controls)\n",
    "df[\"weight\"] = df[\"_weight\"]\n",
    "# Replace zero weights with a small positive value\n",
    "df[\"weight\"] = df[\"weight\"].apply(lambda x: x if x > 0 else 1e-08)\n",
    "#df[\"weight\"] = df[\"weight\"] / df[\"weight\"].mean()\n",
    "# Save weights to file\n",
    "weights_path = f\"Weights_{datetime_string}.csv\"\n",
    "df[[\"ID\", \"weight\"]].to_csv(weights_path, index=False)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "relevant_vars = [\"ID\",\"IDHour\",\"T1ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\",\"ON\", \"log_PMi\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[relevant_vars]\n",
    "df= df.dropna(subset=[\"log_PMi\",\"ON\", \"T1ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"])\n",
    "# Merge weights into original data\n",
    "weights_df = pd.read_csv(weights_path)\n",
    "df = pd.merge(df, weights_df, on=\"ID\", how=\"left\")\n",
    "# Save merged dataset\n",
    "panel_data_path = f\"PanelBDTemuco_{datetime_string}.csv\"\n",
    "df.to_csv(panel_data_path, index=False)\n",
    "# Prepare for panel regression\n",
    "df = df.sort_values(by=[\"ID\", \"IDHour\"])\n",
    "df = df.set_index([\"ID\", \"IDHour\"])\n",
    "\n",
    "model = PanelOLS.from_formula(\n",
    "    f\"{dependent_var} ~ { ' + '.join(independent_vars)} + EntityEffects\",\n",
    "    data=df,\n",
    "    weights=df[\"weight\"],\n",
    "    drop_absorbed=True\n",
    ")\n",
    "results = model.fit(cov_type=\"clustered\", cluster_entity=True)\n",
    "# Extract T1ON results for the current model\n",
    "t1on_result = extract_t1on_results(results, model_name=\"model 3, Nearest Neighbors Matching, n = 3\")\n",
    "t1on_results_df = pd.concat([t1on_results_df, pd.DataFrame([t1on_result])], ignore_index=True)\n",
    "print(results.summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19f5c967-d2f0-44ce-9e85-415f99bbfefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.660393\n",
      "         Iterations 5\n",
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:                log_PMi   R-squared:                        0.4804\n",
      "Estimator:                   PanelOLS   R-squared (Between):              0.7438\n",
      "No. Observations:               17687   R-squared (Within):               0.4804\n",
      "Date:                Sun, Dec 08 2024   R-squared (Overall):              0.7411\n",
      "Time:                        02:04:23   Log-likelihood                   -2643.1\n",
      "Cov. Estimator:             Clustered                                           \n",
      "                                        F-statistic:                      2665.7\n",
      "Entities:                         379   P-value                           0.0000\n",
      "Avg Obs:                       46.668   Distribution:                 F(6,17302)\n",
      "Min Obs:                       7.0000                                           \n",
      "Max Obs:                       49.000   F-statistic (robust):             154.97\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                     865   Distribution:                 F(6,17302)\n",
      "Avg Obs:                       20.447                                           \n",
      "Min Obs:                       1.0000                                           \n",
      "Max Obs:                       35.000                                           \n",
      "                                                                                \n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "ON             0.0506     0.0207     2.4489     0.0143      0.0101      0.0912\n",
      "T1ON          -0.1038     0.0253    -4.1021     0.0000     -0.1534     -0.0542\n",
      "log_PMo        0.5697     0.0222     25.710     0.0000      0.5263      0.6131\n",
      "log_To        -0.0255     0.0153    -1.6701     0.0949     -0.0554      0.0044\n",
      "DayExp        -0.0196     0.0085    -2.3161     0.0206     -0.0361     -0.0030\n",
      "Cycle         -0.0004     0.0051    -0.0882     0.9297     -0.0104      0.0095\n",
      "==============================================================================\n",
      "\n",
      "F-test for Poolability: 20.878\n",
      "P-value: 0.0000\n",
      "Distribution: F(378,17302)\n",
      "\n",
      "Included effects: Entity\n"
     ]
    }
   ],
   "source": [
    "# Perform nearest neighbor matching\n",
    "neighbors = 4\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "# Filter data\n",
    "df = df[df['tper'] < 2]\n",
    "# Define the variables to keep\n",
    "variables_to_keep = [\"ID\",\"T1\",\"Education\", \"GenderHHead1women\", \"Age\", \"Disability\", \"NSize\", \"Anypersonolder60\", \"Chamber\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[variables_to_keep]\n",
    "\n",
    "# Impute missing values with the median of each column\n",
    "columns_to_impute = [\"Education\", \"GenderHHead1women\", \"Age\"]\n",
    "for col in columns_to_impute:\n",
    "    median_value = df[col].median()  # Calculate the median\n",
    "    df[col] = df[col].fillna(median_value)  # Replace missing values with the median\n",
    "# Logistic regression to estimate propensity scores\n",
    "covariates = [\"Education\", \"GenderHHead1women\", \"Age\", \"Disability\", \"NSize\", \"Anypersonolder60\", \"Chamber\"]\n",
    "X = sm.add_constant(df[covariates])\n",
    "y = df[\"T1\"]\n",
    "logit_model = sm.Logit(y, X).fit()\n",
    "df[\"logoddsT1\"] = logit_model.predict(X)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "logit_model = LogisticRegression(max_iter=500)\n",
    "df[\"propensity_score\"] = logit_model.fit(X_scaled, y).predict_proba(X_scaled)[:, 1]\n",
    "# Separate treated and control groups\n",
    "treated = df[df[\"T1\"] == 1]\n",
    "control = df[df[\"T1\"] == 0]\n",
    "\n",
    "# Apply caliper (0.01)\n",
    "caliper = 0.01\n",
    "matches4 = []\n",
    "nn = NearestNeighbors(n_neighbors=neighbors, metric=\"euclidean\")\n",
    "nn.fit(control[[\"propensity_score\"]])\n",
    "distances, indices = nn.kneighbors(treated[[\"propensity_score\"]])\n",
    "matches4 = []\n",
    "for i, dists in enumerate(distances):\n",
    "    matched_indices = indices[i][dists <= caliper]\n",
    "    if len(matched_indices) > 0:\n",
    "        for control_index in matched_indices:\n",
    "            matches4.append((treated.index[i], control.iloc[control_index].name))\n",
    "# Generate matched DataFrame\n",
    "matched_pairs = pd.DataFrame(matches4, columns=[\"treated_index\", \"control_index\"])\n",
    "# Assign weights\n",
    "df[\"_weight\"] = 0.0\n",
    "df.loc[treated.index, \"_weight\"] = 1.0\n",
    "for treated_idx in matched_pairs[\"treated_index\"].unique():\n",
    "    controls = matched_pairs[matched_pairs[\"treated_index\"] == treated_idx][\"control_index\"]\n",
    "    df.loc[controls, \"_weight\"] += 1 / len(controls)\n",
    "df[\"weight\"] = df[\"_weight\"]\n",
    "# Replace zero weights with a small positive value\n",
    "df[\"weight\"] = df[\"weight\"].apply(lambda x: x if x > 0 else 1e-08)\n",
    "#df[\"weight\"] = df[\"weight\"] / df[\"weight\"].mean()\n",
    "# Save weights to file\n",
    "weights_path = f\"Weights_{datetime_string}.csv\"\n",
    "df[[\"ID\", \"weight\"]].to_csv(weights_path, index=False)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "relevant_vars = [\"ID\",\"IDHour\",\"T1ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\",\"ON\", \"log_PMi\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[relevant_vars]\n",
    "df= df.dropna(subset=[\"log_PMi\",\"ON\", \"T1ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"])\n",
    "# Merge weights into original data\n",
    "weights_df = pd.read_csv(weights_path)\n",
    "df = pd.merge(df, weights_df, on=\"ID\", how=\"left\")\n",
    "# Save merged dataset\n",
    "panel_data_path = f\"PanelBDTemuco_{datetime_string}.csv\"\n",
    "df.to_csv(panel_data_path, index=False)\n",
    "# Prepare for panel regression\n",
    "df = df.sort_values(by=[\"ID\", \"IDHour\"])\n",
    "df = df.set_index([\"ID\", \"IDHour\"])\n",
    "\n",
    "model = PanelOLS.from_formula(\n",
    "    f\"{dependent_var} ~ { ' + '.join(independent_vars)} + EntityEffects\",\n",
    "    data=df,\n",
    "    weights=df[\"weight\"],\n",
    "    drop_absorbed=True\n",
    ")\n",
    "results = model.fit(cov_type=\"clustered\", cluster_entity=True)\n",
    "# Extract T1ON results for the current model\n",
    "t1on_result = extract_t1on_results(results, model_name=\"model 4, Nearest Neighbors Matching, n = 4\")\n",
    "t1on_results_df = pd.concat([t1on_results_df, pd.DataFrame([t1on_result])], ignore_index=True)\n",
    "print(results.summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "571e1619-16b8-403f-a5a1-d38e0c9ab514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.660393\n",
      "         Iterations 5\n",
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:                log_PMi   R-squared:                        0.4836\n",
      "Estimator:                   PanelOLS   R-squared (Between):              0.7640\n",
      "No. Observations:               17687   R-squared (Within):               0.4836\n",
      "Date:                Sun, Dec 08 2024   R-squared (Overall):              0.7612\n",
      "Time:                        02:04:24   Log-likelihood                   -2614.0\n",
      "Cov. Estimator:             Clustered                                           \n",
      "                                        F-statistic:                      2700.5\n",
      "Entities:                         379   P-value                           0.0000\n",
      "Avg Obs:                       46.668   Distribution:                 F(6,17302)\n",
      "Min Obs:                       7.0000                                           \n",
      "Max Obs:                       49.000   F-statistic (robust):             162.30\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                     865   Distribution:                 F(6,17302)\n",
      "Avg Obs:                       20.447                                           \n",
      "Min Obs:                       1.0000                                           \n",
      "Max Obs:                       35.000                                           \n",
      "                                                                                \n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "ON             0.0486     0.0212     2.2977     0.0216      0.0071      0.0901\n",
      "T1ON          -0.1023     0.0257    -3.9742     0.0001     -0.1528     -0.0519\n",
      "log_PMo        0.5758     0.0214     26.861     0.0000      0.5338      0.6179\n",
      "log_To        -0.0221     0.0142    -1.5507     0.1210     -0.0499      0.0058\n",
      "DayExp        -0.0173     0.0083    -2.0792     0.0376     -0.0336     -0.0010\n",
      "Cycle         -0.0007     0.0050    -0.1343     0.8931     -0.0105      0.0091\n",
      "==============================================================================\n",
      "\n",
      "F-test for Poolability: 20.821\n",
      "P-value: 0.0000\n",
      "Distribution: F(378,17302)\n",
      "\n",
      "Included effects: Entity\n"
     ]
    }
   ],
   "source": [
    "# Perform nearest neighbor matching\n",
    "neighbors = 5\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "# Filter data\n",
    "df = df[df['tper'] < 2]\n",
    "# Define the variables to keep\n",
    "variables_to_keep = [\"ID\",\"T1\",\"Education\", \"GenderHHead1women\", \"Age\", \"Disability\", \"NSize\", \"Anypersonolder60\", \"Chamber\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[variables_to_keep]\n",
    "\n",
    "# Impute missing values with the median of each column\n",
    "columns_to_impute = [\"Education\", \"GenderHHead1women\", \"Age\"]\n",
    "for col in columns_to_impute:\n",
    "    median_value = df[col].median()  # Calculate the median\n",
    "    df[col] = df[col].fillna(median_value)  # Replace missing values with the median\n",
    "# Logistic regression to estimate propensity scores\n",
    "covariates = [\"Education\", \"GenderHHead1women\", \"Age\", \"Disability\", \"NSize\", \"Anypersonolder60\", \"Chamber\"]\n",
    "X = sm.add_constant(df[covariates])\n",
    "y = df[\"T1\"]\n",
    "logit_model = sm.Logit(y, X).fit()\n",
    "df[\"logoddsT1\"] = logit_model.predict(X)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "logit_model = LogisticRegression(max_iter=500)\n",
    "df[\"propensity_score\"] = logit_model.fit(X_scaled, y).predict_proba(X_scaled)[:, 1]\n",
    "# Separate treated and control groups\n",
    "treated = df[df[\"T1\"] == 1]\n",
    "control = df[df[\"T1\"] == 0]\n",
    "\n",
    "# Apply caliper (0.01)\n",
    "caliper = 0.01\n",
    "matches4 = []\n",
    "nn = NearestNeighbors(n_neighbors=neighbors, metric=\"euclidean\")\n",
    "nn.fit(control[[\"propensity_score\"]])\n",
    "distances, indices = nn.kneighbors(treated[[\"propensity_score\"]])\n",
    "matches4 = []\n",
    "for i, dists in enumerate(distances):\n",
    "    matched_indices = indices[i][dists <= caliper]\n",
    "    if len(matched_indices) > 0:\n",
    "        for control_index in matched_indices:\n",
    "            matches4.append((treated.index[i], control.iloc[control_index].name))\n",
    "# Generate matched DataFrame\n",
    "matched_pairs = pd.DataFrame(matches4, columns=[\"treated_index\", \"control_index\"])\n",
    "# Assign weights\n",
    "df[\"_weight\"] = 0.0\n",
    "df.loc[treated.index, \"_weight\"] = 1.0\n",
    "for treated_idx in matched_pairs[\"treated_index\"].unique():\n",
    "    controls = matched_pairs[matched_pairs[\"treated_index\"] == treated_idx][\"control_index\"]\n",
    "    df.loc[controls, \"_weight\"] += 1 / len(controls)\n",
    "df[\"weight\"] = df[\"_weight\"]\n",
    "# Replace zero weights with a small positive value\n",
    "df[\"weight\"] = df[\"weight\"].apply(lambda x: x if x > 0 else 1e-08)\n",
    "#df[\"weight\"] = df[\"weight\"] / df[\"weight\"].mean()\n",
    "# Save weights to file\n",
    "weights_path = f\"Weights_{datetime_string}.csv\"\n",
    "df[[\"ID\", \"weight\"]].to_csv(weights_path, index=False)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "relevant_vars = [\"ID\",\"IDHour\",\"T1ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\",\"ON\", \"log_PMi\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[relevant_vars]\n",
    "df= df.dropna(subset=[\"log_PMi\",\"ON\", \"T1ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"])\n",
    "# Merge weights into original data\n",
    "weights_df = pd.read_csv(weights_path)\n",
    "df = pd.merge(df, weights_df, on=\"ID\", how=\"left\")\n",
    "# Save merged dataset\n",
    "panel_data_path = f\"PanelBDTemuco_{datetime_string}.csv\"\n",
    "df.to_csv(panel_data_path, index=False)\n",
    "# Prepare for panel regression\n",
    "df = df.sort_values(by=[\"ID\", \"IDHour\"])\n",
    "df = df.set_index([\"ID\", \"IDHour\"])\n",
    "\n",
    "model = PanelOLS.from_formula(\n",
    "    f\"{dependent_var} ~ { ' + '.join(independent_vars)} + EntityEffects\",\n",
    "    data=df,\n",
    "    weights=df[\"weight\"],\n",
    "    drop_absorbed=True\n",
    ")\n",
    "results = model.fit(cov_type=\"clustered\", cluster_entity=True)\n",
    "# Extract T1ON results for the current model\n",
    "t1on_result = extract_t1on_results(results, model_name=\"model 5, Nearest Neighbors Matching, n = 5\")\n",
    "t1on_results_df = pd.concat([t1on_results_df, pd.DataFrame([t1on_result])], ignore_index=True)\n",
    "print(results.summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eac0ab0c-5692-4c35-94f7-f3fec6c1b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.660393\n",
      "         Iterations 5\n",
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:                log_PMi   R-squared:                        0.4860\n",
      "Estimator:                   PanelOLS   R-squared (Between):              0.7802\n",
      "No. Observations:               17687   R-squared (Within):               0.4860\n",
      "Date:                Sun, Dec 08 2024   R-squared (Overall):              0.7772\n",
      "Time:                        02:04:24   Log-likelihood                   -2623.7\n",
      "Cov. Estimator:             Clustered                                           \n",
      "                                        F-statistic:                      2726.1\n",
      "Entities:                         379   P-value                           0.0000\n",
      "Avg Obs:                       46.668   Distribution:                 F(6,17302)\n",
      "Min Obs:                       7.0000                                           \n",
      "Max Obs:                       49.000   F-statistic (robust):             165.19\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                     865   Distribution:                 F(6,17302)\n",
      "Avg Obs:                       20.447                                           \n",
      "Min Obs:                       1.0000                                           \n",
      "Max Obs:                       35.000                                           \n",
      "                                                                                \n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "ON             0.0469     0.0207     2.2654     0.0235      0.0063      0.0874\n",
      "T1ON          -0.1010     0.0251    -4.0254     0.0001     -0.1502     -0.0518\n",
      "log_PMo        0.5784     0.0208     27.817     0.0000      0.5376      0.6191\n",
      "log_To        -0.0187     0.0141    -1.3243     0.1854     -0.0464      0.0090\n",
      "DayExp        -0.0151     0.0082    -1.8445     0.0651     -0.0312      0.0009\n",
      "Cycle         -0.0005     0.0050    -0.1035     0.9176     -0.0102      0.0092\n",
      "==============================================================================\n",
      "\n",
      "F-test for Poolability: 20.928\n",
      "P-value: 0.0000\n",
      "Distribution: F(378,17302)\n",
      "\n",
      "Included effects: Entity\n"
     ]
    }
   ],
   "source": [
    "# Perform nearest neighbor matching\n",
    "neighbors = 6\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "# Filter data\n",
    "df = df[df['tper'] < 2]\n",
    "# Define the variables to keep\n",
    "variables_to_keep = [\"ID\",\"T1\",\"Education\", \"GenderHHead1women\", \"Age\", \"Disability\", \"NSize\", \"Anypersonolder60\", \"Chamber\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[variables_to_keep]\n",
    "\n",
    "# Impute missing values with the median of each column\n",
    "columns_to_impute = [\"Education\", \"GenderHHead1women\", \"Age\"]\n",
    "for col in columns_to_impute:\n",
    "    median_value = df[col].median()  # Calculate the median\n",
    "    df[col] = df[col].fillna(median_value)  # Replace missing values with the median\n",
    "# Logistic regression to estimate propensity scores\n",
    "covariates = [\"Education\", \"GenderHHead1women\", \"Age\", \"Disability\", \"NSize\", \"Anypersonolder60\", \"Chamber\"]\n",
    "X = sm.add_constant(df[covariates])\n",
    "y = df[\"T1\"]\n",
    "logit_model = sm.Logit(y, X).fit()\n",
    "df[\"logoddsT1\"] = logit_model.predict(X)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "logit_model = LogisticRegression(max_iter=500)\n",
    "df[\"propensity_score\"] = logit_model.fit(X_scaled, y).predict_proba(X_scaled)[:, 1]\n",
    "# Separate treated and control groups\n",
    "treated = df[df[\"T1\"] == 1]\n",
    "control = df[df[\"T1\"] == 0]\n",
    "\n",
    "# Apply caliper (0.01)\n",
    "caliper = 0.01\n",
    "matches4 = []\n",
    "nn = NearestNeighbors(n_neighbors=neighbors, metric=\"euclidean\")\n",
    "nn.fit(control[[\"propensity_score\"]])\n",
    "distances, indices = nn.kneighbors(treated[[\"propensity_score\"]])\n",
    "matches4 = []\n",
    "for i, dists in enumerate(distances):\n",
    "    matched_indices = indices[i][dists <= caliper]\n",
    "    if len(matched_indices) > 0:\n",
    "        for control_index in matched_indices:\n",
    "            matches4.append((treated.index[i], control.iloc[control_index].name))\n",
    "# Generate matched DataFrame\n",
    "matched_pairs = pd.DataFrame(matches4, columns=[\"treated_index\", \"control_index\"])\n",
    "# Assign weights\n",
    "df[\"_weight\"] = 0.0\n",
    "df.loc[treated.index, \"_weight\"] = 1.0\n",
    "for treated_idx in matched_pairs[\"treated_index\"].unique():\n",
    "    controls = matched_pairs[matched_pairs[\"treated_index\"] == treated_idx][\"control_index\"]\n",
    "    df.loc[controls, \"_weight\"] += 1 / len(controls)\n",
    "df[\"weight\"] = df[\"_weight\"]\n",
    "# Replace zero weights with a small positive value\n",
    "df[\"weight\"] = df[\"weight\"].apply(lambda x: x if x > 0 else 1e-08)\n",
    "#df[\"weight\"] = df[\"weight\"] / df[\"weight\"].mean()\n",
    "# Save weights to file\n",
    "weights_path = f\"Weights_{datetime_string}.csv\"\n",
    "df[[\"ID\", \"weight\"]].to_csv(weights_path, index=False)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "relevant_vars = [\"ID\",\"IDHour\",\"T1ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\",\"ON\", \"log_PMi\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[relevant_vars]\n",
    "df= df.dropna(subset=[\"log_PMi\",\"ON\", \"T1ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"])\n",
    "# Merge weights into original data\n",
    "weights_df = pd.read_csv(weights_path)\n",
    "df = pd.merge(df, weights_df, on=\"ID\", how=\"left\")\n",
    "# Save merged dataset\n",
    "panel_data_path = f\"PanelBDTemuco_{datetime_string}.csv\"\n",
    "df.to_csv(panel_data_path, index=False)\n",
    "# Prepare for panel regression\n",
    "df = df.sort_values(by=[\"ID\", \"IDHour\"])\n",
    "df = df.set_index([\"ID\", \"IDHour\"])\n",
    "\n",
    "model = PanelOLS.from_formula(\n",
    "    f\"{dependent_var} ~ { ' + '.join(independent_vars)} + EntityEffects\",\n",
    "    data=df,\n",
    "    weights=df[\"weight\"],\n",
    "    drop_absorbed=True\n",
    ")\n",
    "results = model.fit(cov_type=\"clustered\", cluster_entity=True)\n",
    "# Extract T1ON results for the current model\n",
    "t1on_result = extract_t1on_results(results, model_name=\"model 6, Nearest Neighbors Matching, n = 6\")\n",
    "t1on_results_df = pd.concat([t1on_results_df, pd.DataFrame([t1on_result])], ignore_index=True)\n",
    "print(results.summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caf7360-6e72-45ab-a8ac-f80f3d97596e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f9d863f-9275-4645-b010-d89155a1f853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>PELLET*ON Coefficient</th>\n",
       "      <th>Std. Error</th>\n",
       "      <th>T-stat</th>\n",
       "      <th>P-value</th>\n",
       "      <th>CI Lower</th>\n",
       "      <th>CI Upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model 1 - Without Matching</td>\n",
       "      <td>-0.127517</td>\n",
       "      <td>0.022136</td>\n",
       "      <td>-5.760743</td>\n",
       "      <td>8.516641e-09</td>\n",
       "      <td>-0.170903</td>\n",
       "      <td>-0.084132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model 2, Nearest Neighbors Matching, n = 2</td>\n",
       "      <td>-0.092523</td>\n",
       "      <td>0.024358</td>\n",
       "      <td>-3.798535</td>\n",
       "      <td>1.460518e-04</td>\n",
       "      <td>-0.140264</td>\n",
       "      <td>-0.044782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model 3, Nearest Neighbors Matching, n = 3</td>\n",
       "      <td>-0.097194</td>\n",
       "      <td>0.025724</td>\n",
       "      <td>-3.778353</td>\n",
       "      <td>1.583980e-04</td>\n",
       "      <td>-0.147613</td>\n",
       "      <td>-0.046775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model 4, Nearest Neighbors Matching, n = 4</td>\n",
       "      <td>-0.103776</td>\n",
       "      <td>0.025298</td>\n",
       "      <td>-4.102129</td>\n",
       "      <td>4.112390e-05</td>\n",
       "      <td>-0.153360</td>\n",
       "      <td>-0.054192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model 5, Nearest Neighbors Matching, n = 5</td>\n",
       "      <td>-0.102311</td>\n",
       "      <td>0.025744</td>\n",
       "      <td>-3.974199</td>\n",
       "      <td>7.090315e-05</td>\n",
       "      <td>-0.152768</td>\n",
       "      <td>-0.051853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>model 6, Nearest Neighbors Matching, n = 6</td>\n",
       "      <td>-0.101040</td>\n",
       "      <td>0.025101</td>\n",
       "      <td>-4.025353</td>\n",
       "      <td>5.713230e-05</td>\n",
       "      <td>-0.150237</td>\n",
       "      <td>-0.051842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Model  PELLET*ON Coefficient  \\\n",
       "0                  model 1 - Without Matching              -0.127517   \n",
       "1  model 2, Nearest Neighbors Matching, n = 2              -0.092523   \n",
       "2  model 3, Nearest Neighbors Matching, n = 3              -0.097194   \n",
       "3  model 4, Nearest Neighbors Matching, n = 4              -0.103776   \n",
       "4  model 5, Nearest Neighbors Matching, n = 5              -0.102311   \n",
       "5  model 6, Nearest Neighbors Matching, n = 6              -0.101040   \n",
       "\n",
       "   Std. Error    T-stat       P-value  CI Lower  CI Upper  \n",
       "0    0.022136 -5.760743  8.516641e-09 -0.170903 -0.084132  \n",
       "1    0.024358 -3.798535  1.460518e-04 -0.140264 -0.044782  \n",
       "2    0.025724 -3.778353  1.583980e-04 -0.147613 -0.046775  \n",
       "3    0.025298 -4.102129  4.112390e-05 -0.153360 -0.054192  \n",
       "4    0.025744 -3.974199  7.090315e-05 -0.152768 -0.051853  \n",
       "5    0.025101 -4.025353  5.713230e-05 -0.150237 -0.051842  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1on_results_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3717a34-f0cd-4284-b480-77c70f9da29c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a778a91f-e32e-4d89-9c15-53fe4fc3cc5c",
   "metadata": {},
   "source": [
    "Kernel Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54bad950-3991-42b4-a324-a7e95e77284b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:                log_PMi   R-squared:                        0.4315\n",
      "Estimator:                   PanelOLS   R-squared (Between):              0.6731\n",
      "No. Observations:               17687   R-squared (Within):               0.4315\n",
      "Date:                Sun, Dec 08 2024   R-squared (Overall):              0.6706\n",
      "Time:                        02:04:27   Log-likelihood                   -3368.7\n",
      "Cov. Estimator:             Clustered                                           \n",
      "                                        F-statistic:                      2188.4\n",
      "Entities:                         379   P-value                           0.0000\n",
      "Avg Obs:                       46.668   Distribution:                 F(6,17302)\n",
      "Min Obs:                       7.0000                                           \n",
      "Max Obs:                       49.000   F-statistic (robust):             67.545\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                     865   Distribution:                 F(6,17302)\n",
      "Avg Obs:                       20.447                                           \n",
      "Min Obs:                       1.0000                                           \n",
      "Max Obs:                       35.000                                           \n",
      "                                                                                \n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "ON             0.0533     0.0207     2.5792     0.0099      0.0128      0.0938\n",
      "T1ON          -0.1245     0.0243    -5.1152     0.0000     -0.1722     -0.0768\n",
      "log_PMo        0.5284     0.0324     16.300     0.0000      0.4648      0.5919\n",
      "log_To         0.0032     0.0218     0.1473     0.8829     -0.0394      0.0458\n",
      "DayExp        -0.0278     0.0122    -2.2728     0.0230     -0.0517     -0.0038\n",
      "Cycle          0.0089     0.0062     1.4348     0.1514     -0.0033      0.0212\n",
      "==============================================================================\n",
      "\n",
      "F-test for Poolability: 19.969\n",
      "P-value: 0.0000\n",
      "Distribution: F(378,17302)\n",
      "\n",
      "Included effects: Entity\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "# Filter data\n",
    "df = df[df['tper'] < 2]\n",
    "variables_to_keep = [\"ID\", \"T1\", \"Education\", \"GenderHHead1women\", \"Age\", \"Disability\", \"NSize\", \"Anypersonolder60\", \"Chamber\"]\n",
    "df = df[variables_to_keep]\n",
    "# Impute missing values with the median\n",
    "columns_to_impute = [\"Education\", \"GenderHHead1women\", \"Age\"]\n",
    "for col in columns_to_impute:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "# Logistic regression to estimate propensity scores\n",
    "covariates = [\"Education\", \"GenderHHead1women\", \"Age\", \"Disability\", \"NSize\", \"Anypersonolder60\", \"Chamber\"]\n",
    "X = df[covariates]\n",
    "y = df[\"T1\"]\n",
    "# Standardize covariates\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "logit_model = LogisticRegression(max_iter=500)\n",
    "df[\"propensity_score\"] = logit_model.fit(X_scaled, y).predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "# Separate treated and control groups\n",
    "treated = df[df[\"T1\"] == 1]\n",
    "control = df[df[\"T1\"] == 0]\n",
    "\n",
    "# Perform kernel matching using Gaussian kernel\n",
    "bandwidth = 0.1  # Adjust bandwidth as needed\n",
    "distances = pairwise_distances(treated[[\"propensity_score\"]], control[[\"propensity_score\"]], metric=\"euclidean\")\n",
    "kernel_weights = np.exp(-distances**2 / (2 * bandwidth**2))\n",
    "# Assign weights for control units\n",
    "df[\"_weight\"] = 0.0\n",
    "df.loc[treated.index, \"_weight\"] = 1.0  # Full weight for treated units\n",
    "for i, treated_idx in enumerate(treated.index):\n",
    "    for j, control_idx in enumerate(control.index):\n",
    "        df.loc[control_idx, \"_weight\"] += kernel_weights[i, j]\n",
    "df[\"weight\"] = df[\"_weight\"]\n",
    "# Replace zero weights with a small positive value\n",
    "df[\"weight\"] = df[\"weight\"].apply(lambda x: x if x > 0 else 1e-08)\n",
    "# Save weights to a file\n",
    "weights_path = f\"Weights_{datetime_string}.csv\"\n",
    "df[[\"ID\", \"weight\"]].to_csv(weights_path, index=False)\n",
    "# Load the original dataset for panel regression\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "relevant_vars = [\"ID\",\"IDHour\",\"T1ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\",\"ON\", \"log_PMi\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[relevant_vars]\n",
    "df= df.dropna(subset=[\"log_PMi\",\"ON\", \"T1ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"])\n",
    "# Merge weights into original data\n",
    "weights_df = pd.read_csv(weights_path)\n",
    "df = pd.merge(df, weights_df, on=\"ID\", how=\"left\")\n",
    "# Save merged dataset\n",
    "panel_data_path = f\"PanelBDTemuco_{datetime_string}.csv\"\n",
    "df.to_csv(panel_data_path, index=False)\n",
    "# Prepare for panel regression\n",
    "df = df.sort_values(by=[\"ID\", \"IDHour\"])\n",
    "df = df.set_index([\"ID\", \"IDHour\"])\n",
    "# Fixed Effects Model without weights\n",
    "dependent_var = \"log_PMi\"\n",
    "independent_vars = [\"ON\", \"T1ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"]\n",
    "\n",
    "# Fixed Effects Model with weights\n",
    "model = PanelOLS.from_formula(\n",
    "    f\"{dependent_var} ~ { ' + '.join(independent_vars)} + EntityEffects\",\n",
    "    data=df,\n",
    "    weights=df[\"weight\"],\n",
    "    drop_absorbed=True\n",
    ")\n",
    "results = model.fit(cov_type=\"clustered\", cluster_entity=True)\n",
    "# Extract T1ON results for the current model\n",
    "t1on_result = extract_t1on_results(results, model_name=\"model Kernel Matching, bandwidth = 0.1\")\n",
    "t1on_results_df = pd.concat([t1on_results_df, pd.DataFrame([t1on_result])], ignore_index=True)\n",
    "print(results.summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f68e7da-cf16-4b0e-a3eb-df834321dc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:                log_PMi   R-squared:                        0.4413\n",
      "Estimator:                   PanelOLS   R-squared (Between):              0.7011\n",
      "No. Observations:               17687   R-squared (Within):               0.4413\n",
      "Date:                Sun, Dec 08 2024   R-squared (Overall):              0.6985\n",
      "Time:                        02:04:32   Log-likelihood                   -3274.2\n",
      "Cov. Estimator:             Clustered                                           \n",
      "                                        F-statistic:                      2278.0\n",
      "Entities:                         379   P-value                           0.0000\n",
      "Avg Obs:                       46.668   Distribution:                 F(6,17302)\n",
      "Min Obs:                       7.0000                                           \n",
      "Max Obs:                       49.000   F-statistic (robust):             79.159\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                     865   Distribution:                 F(6,17302)\n",
      "Avg Obs:                       20.447                                           \n",
      "Min Obs:                       1.0000                                           \n",
      "Max Obs:                       35.000                                           \n",
      "                                                                                \n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "ON             0.0543     0.0193     2.8122     0.0049      0.0164      0.0921\n",
      "T1ON          -0.1266     0.0232    -5.4559     0.0000     -0.1721     -0.0811\n",
      "log_PMo        0.5401     0.0303     17.805     0.0000      0.4806      0.5996\n",
      "log_To         0.0064     0.0204     0.3127     0.7545     -0.0336      0.0464\n",
      "DayExp        -0.0260     0.0114    -2.2694     0.0233     -0.0484     -0.0035\n",
      "Cycle          0.0084     0.0059     1.4250     0.1542     -0.0032      0.0201\n",
      "==============================================================================\n",
      "\n",
      "F-test for Poolability: 20.260\n",
      "P-value: 0.0000\n",
      "Distribution: F(378,17302)\n",
      "\n",
      "Included effects: Entity\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "# Filter data\n",
    "df = df[df['tper'] < 2]\n",
    "variables_to_keep = [\"ID\", \"T1\", \"Education\", \"GenderHHead1women\", \"Age\", \"Disability\", \"NSize\", \"Anypersonolder60\", \"Chamber\"]\n",
    "df = df[variables_to_keep]\n",
    "# Impute missing values with the median\n",
    "columns_to_impute = [\"Education\", \"GenderHHead1women\", \"Age\"]\n",
    "for col in columns_to_impute:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "# Logistic regression to estimate propensity scores\n",
    "covariates = [\"Education\", \"GenderHHead1women\", \"Age\", \"Disability\", \"NSize\", \"Anypersonolder60\", \"Chamber\"]\n",
    "X = df[covariates]\n",
    "y = df[\"T1\"]\n",
    "# Standardize covariates\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "logit_model = LogisticRegression(max_iter=500)\n",
    "df[\"propensity_score\"] = logit_model.fit(X_scaled, y).predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "# Separate treated and control groups\n",
    "treated = df[df[\"T1\"] == 1]\n",
    "control = df[df[\"T1\"] == 0]\n",
    "\n",
    "# Perform kernel matching using Gaussian kernel\n",
    "bandwidth = 0.2  # Adjust bandwidth as needed\n",
    "distances = pairwise_distances(treated[[\"propensity_score\"]], control[[\"propensity_score\"]], metric=\"euclidean\")\n",
    "kernel_weights = np.exp(-distances**2 / (2 * bandwidth**2))\n",
    "# Assign weights for control units\n",
    "df[\"_weight\"] = 0.0\n",
    "df.loc[treated.index, \"_weight\"] = 1.0  # Full weight for treated units\n",
    "for i, treated_idx in enumerate(treated.index):\n",
    "    for j, control_idx in enumerate(control.index):\n",
    "        df.loc[control_idx, \"_weight\"] += kernel_weights[i, j]\n",
    "df[\"weight\"] = df[\"_weight\"]\n",
    "# Replace zero weights with a small positive value\n",
    "df[\"weight\"] = df[\"weight\"].apply(lambda x: x if x > 0 else 1e-08)\n",
    "# Save weights to a file\n",
    "weights_path = f\"Weights_{datetime_string}.csv\"\n",
    "df[[\"ID\", \"weight\"]].to_csv(weights_path, index=False)\n",
    "# Load the original dataset for panel regression\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "relevant_vars = [\"ID\",\"IDHour\",\"T1ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\",\"ON\", \"log_PMi\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[relevant_vars]\n",
    "df= df.dropna(subset=[\"log_PMi\",\"ON\", \"T1ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"])\n",
    "# Merge weights into original data\n",
    "weights_df = pd.read_csv(weights_path)\n",
    "df = pd.merge(df, weights_df, on=\"ID\", how=\"left\")\n",
    "# Save merged dataset\n",
    "panel_data_path = f\"PanelBDTemuco_{datetime_string}.csv\"\n",
    "df.to_csv(panel_data_path, index=False)\n",
    "# Prepare for panel regression\n",
    "df = df.sort_values(by=[\"ID\", \"IDHour\"])\n",
    "df = df.set_index([\"ID\", \"IDHour\"])\n",
    "# Fixed Effects Model without weights\n",
    "dependent_var = \"log_PMi\"\n",
    "independent_vars = [\"ON\", \"T1ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"]\n",
    "\n",
    "# Fixed Effects Model with weights\n",
    "model = PanelOLS.from_formula(\n",
    "    f\"{dependent_var} ~ { ' + '.join(independent_vars)} + EntityEffects\",\n",
    "    data=df,\n",
    "    weights=df[\"weight\"],\n",
    "    drop_absorbed=True\n",
    ")\n",
    "results = model.fit(cov_type=\"clustered\", cluster_entity=True)\n",
    "# Extract T1ON results for the current model\n",
    "t1on_result = extract_t1on_results(results, model_name=\"model Kernel Matching, bandwidth = 0.2\")\n",
    "t1on_results_df = pd.concat([t1on_results_df, pd.DataFrame([t1on_result])], ignore_index=True)\n",
    "print(results.summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0ce07ee-e798-40ce-ade7-49c060e6dffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:                log_PMi   R-squared:                        0.4470\n",
      "Estimator:                   PanelOLS   R-squared (Between):              0.7143\n",
      "No. Observations:               17687   R-squared (Within):               0.4470\n",
      "Date:                Sun, Dec 08 2024   R-squared (Overall):              0.7116\n",
      "Time:                        02:04:37   Log-likelihood                   -3217.8\n",
      "Cov. Estimator:             Clustered                                           \n",
      "                                        F-statistic:                      2331.2\n",
      "Entities:                         379   P-value                           0.0000\n",
      "Avg Obs:                       46.668   Distribution:                 F(6,17302)\n",
      "Min Obs:                       7.0000                                           \n",
      "Max Obs:                       49.000   F-statistic (robust):             85.026\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                     865   Distribution:                 F(6,17302)\n",
      "Avg Obs:                       20.447                                           \n",
      "Min Obs:                       1.0000                                           \n",
      "Max Obs:                       35.000                                           \n",
      "                                                                                \n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "ON             0.0551     0.0188     2.9329     0.0034      0.0183      0.0918\n",
      "T1ON          -0.1281     0.0228    -5.6294     0.0000     -0.1727     -0.0835\n",
      "log_PMo        0.5461     0.0295     18.515     0.0000      0.4883      0.6039\n",
      "log_To         0.0085     0.0199     0.4263     0.6699     -0.0305      0.0475\n",
      "DayExp        -0.0252     0.0111    -2.2665     0.0234     -0.0470     -0.0034\n",
      "Cycle          0.0081     0.0058     1.4012     0.1612     -0.0032      0.0195\n",
      "==============================================================================\n",
      "\n",
      "F-test for Poolability: 20.474\n",
      "P-value: 0.0000\n",
      "Distribution: F(378,17302)\n",
      "\n",
      "Included effects: Entity\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "# Filter data\n",
    "df = df[df['tper'] < 2]\n",
    "variables_to_keep = [\"ID\", \"T1\", \"Education\", \"GenderHHead1women\", \"Age\", \"Disability\", \"NSize\", \"Anypersonolder60\", \"Chamber\"]\n",
    "df = df[variables_to_keep]\n",
    "# Impute missing values with the median\n",
    "columns_to_impute = [\"Education\", \"GenderHHead1women\", \"Age\"]\n",
    "for col in columns_to_impute:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "# Logistic regression to estimate propensity scores\n",
    "covariates = [\"Education\", \"GenderHHead1women\", \"Age\", \"Disability\", \"NSize\", \"Anypersonolder60\", \"Chamber\"]\n",
    "X = df[covariates]\n",
    "y = df[\"T1\"]\n",
    "# Standardize covariates\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "logit_model = LogisticRegression(max_iter=500)\n",
    "df[\"propensity_score\"] = logit_model.fit(X_scaled, y).predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "# Separate treated and control groups\n",
    "treated = df[df[\"T1\"] == 1]\n",
    "control = df[df[\"T1\"] == 0]\n",
    "\n",
    "# Perform kernel matching using Gaussian kernel\n",
    "bandwidth = 0.3  # Adjust bandwidth as needed\n",
    "distances = pairwise_distances(treated[[\"propensity_score\"]], control[[\"propensity_score\"]], metric=\"euclidean\")\n",
    "kernel_weights = np.exp(-distances**2 / (2 * bandwidth**2))\n",
    "# Assign weights for control units\n",
    "df[\"_weight\"] = 0.0\n",
    "df.loc[treated.index, \"_weight\"] = 1.0  # Full weight for treated units\n",
    "for i, treated_idx in enumerate(treated.index):\n",
    "    for j, control_idx in enumerate(control.index):\n",
    "        df.loc[control_idx, \"_weight\"] += kernel_weights[i, j]\n",
    "df[\"weight\"] = df[\"_weight\"]\n",
    "# Replace zero weights with a small positive value\n",
    "df[\"weight\"] = df[\"weight\"].apply(lambda x: x if x > 0 else 1e-08)\n",
    "# Save weights to a file\n",
    "weights_path = f\"Weights_{datetime_string}.csv\"\n",
    "df[[\"ID\", \"weight\"]].to_csv(weights_path, index=False)\n",
    "# Load the original dataset for panel regression\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "relevant_vars = [\"ID\",\"IDHour\",\"T1ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\",\"ON\", \"log_PMi\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[relevant_vars]\n",
    "df= df.dropna(subset=[\"log_PMi\",\"ON\", \"T1ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"])\n",
    "# Merge weights into original data\n",
    "weights_df = pd.read_csv(weights_path)\n",
    "df = pd.merge(df, weights_df, on=\"ID\", how=\"left\")\n",
    "# Save merged dataset\n",
    "panel_data_path = f\"PanelBDTemuco_{datetime_string}.csv\"\n",
    "df.to_csv(panel_data_path, index=False)\n",
    "# Prepare for panel regression\n",
    "df = df.sort_values(by=[\"ID\", \"IDHour\"])\n",
    "df = df.set_index([\"ID\", \"IDHour\"])\n",
    "# Fixed Effects Model without weights\n",
    "dependent_var = \"log_PMi\"\n",
    "independent_vars = [\"ON\", \"T1ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"]\n",
    "\n",
    "# Fixed Effects Model with weights\n",
    "model = PanelOLS.from_formula(\n",
    "    f\"{dependent_var} ~ { ' + '.join(independent_vars)} + EntityEffects\",\n",
    "    data=df,\n",
    "    weights=df[\"weight\"],\n",
    "    drop_absorbed=True\n",
    ")\n",
    "results = model.fit(cov_type=\"clustered\", cluster_entity=True)\n",
    "# Extract T1ON results for the current model\n",
    "t1on_result = extract_t1on_results(results, model_name=\"model Kernel Matching, bandwidth = 0.3\")\n",
    "t1on_results_df = pd.concat([t1on_results_df, pd.DataFrame([t1on_result])], ignore_index=True)\n",
    "print(results.summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90669be-4c24-414b-91c5-f394d726959b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "713a0feb-863e-4e3e-be5d-cc6be7f5a804",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1on_results_df = t1on_results_df.drop(index=[1,2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3591315-829f-4aed-9f85-dd299b7cc235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>PELLET*ON Coefficient</th>\n",
       "      <th>Std. Error</th>\n",
       "      <th>T-stat</th>\n",
       "      <th>P-value</th>\n",
       "      <th>CI Lower</th>\n",
       "      <th>CI Upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model 1 - Without Matching</td>\n",
       "      <td>-0.127517</td>\n",
       "      <td>0.022136</td>\n",
       "      <td>-5.760743</td>\n",
       "      <td>8.516641e-09</td>\n",
       "      <td>-0.170903</td>\n",
       "      <td>-0.084132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>model Kernel Matching, bandwidth = 0.1</td>\n",
       "      <td>-0.124490</td>\n",
       "      <td>0.024337</td>\n",
       "      <td>-5.115189</td>\n",
       "      <td>3.167751e-07</td>\n",
       "      <td>-0.172191</td>\n",
       "      <td>-0.076789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model Kernel Matching, bandwidth = 0.2</td>\n",
       "      <td>-0.126606</td>\n",
       "      <td>0.023205</td>\n",
       "      <td>-5.455863</td>\n",
       "      <td>4.940471e-08</td>\n",
       "      <td>-0.172088</td>\n",
       "      <td>-0.081123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>model Kernel Matching, bandwidth = 0.3</td>\n",
       "      <td>-0.128121</td>\n",
       "      <td>0.022759</td>\n",
       "      <td>-5.629414</td>\n",
       "      <td>1.836286e-08</td>\n",
       "      <td>-0.172730</td>\n",
       "      <td>-0.083513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  PELLET*ON Coefficient  Std. Error  \\\n",
       "0              model 1 - Without Matching              -0.127517    0.022136   \n",
       "6  model Kernel Matching, bandwidth = 0.1              -0.124490    0.024337   \n",
       "7  model Kernel Matching, bandwidth = 0.2              -0.126606    0.023205   \n",
       "8  model Kernel Matching, bandwidth = 0.3              -0.128121    0.022759   \n",
       "\n",
       "     T-stat       P-value  CI Lower  CI Upper  \n",
       "0 -5.760743  8.516641e-09 -0.170903 -0.084132  \n",
       "6 -5.115189  3.167751e-07 -0.172191 -0.076789  \n",
       "7 -5.455863  4.940471e-08 -0.172088 -0.081123  \n",
       "8 -5.629414  1.836286e-08 -0.172730 -0.083513  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1on_results_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "066ff4fe-cc29-440c-8845-3f292fd9bff9",
   "metadata": {},
   "source": [
    "#### Balance matching Table  T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59dc1abe-a47f-4940-a8d1-3f2f718c60ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.660393\n",
      "         Iterations 5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from linearmodels.panel import PanelOLS\n",
    "# Set datetime string\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import pairwise_distances\n",
    "datetime_string = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Perform nearest neighbor matching\n",
    "neighbors = 4\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "# Filter data\n",
    "df = df[df['tper'] < 2]\n",
    "# Define the variables to keep\n",
    "variables_to_keep = [\"ID\",\"T1\",\"Education\", \"GenderHHead1women\", \"Age\", \"Disability\", \"NSize\", \"Anypersonolder60\", \"Chamber\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[variables_to_keep]\n",
    "\n",
    "# Impute missing values with the median of each column\n",
    "columns_to_impute = [\"Education\", \"GenderHHead1women\", \"Age\"]\n",
    "for col in columns_to_impute:\n",
    "    median_value = df[col].median()  # Calculate the median\n",
    "    df[col] = df[col].fillna(median_value)  # Replace missing values with the median\n",
    "# Logistic regression to estimate propensity scores\n",
    "covariates = [\"Education\", \"GenderHHead1women\", \"Age\", \"Disability\", \"NSize\", \"Anypersonolder60\", \"Chamber\"]\n",
    "X = sm.add_constant(df[covariates])\n",
    "y = df[\"T1\"]\n",
    "logit_model = sm.Logit(y, X).fit()\n",
    "df[\"logoddsT1\"] = logit_model.predict(X)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "logit_model = LogisticRegression(max_iter=500)\n",
    "df[\"propensity_score\"] = logit_model.fit(X_scaled, y).predict_proba(X_scaled)[:, 1]\n",
    "# Separate treated and control groups\n",
    "treated = df[df[\"T1\"] == 1]\n",
    "control = df[df[\"T1\"] == 0]\n",
    "\n",
    "# Apply caliper (0.01)\n",
    "caliper = 0.01\n",
    "matches4 = []\n",
    "nn = NearestNeighbors(n_neighbors=neighbors, metric=\"euclidean\")\n",
    "nn.fit(control[[\"propensity_score\"]])\n",
    "distances, indices = nn.kneighbors(treated[[\"propensity_score\"]])\n",
    "matches4 = []\n",
    "for i, dists in enumerate(distances):\n",
    "    matched_indices = indices[i][dists <= caliper]\n",
    "    if len(matched_indices) > 0:\n",
    "        for control_index in matched_indices:\n",
    "            matches4.append((treated.index[i], control.iloc[control_index].name))\n",
    "# Generate matched DataFrame\n",
    "matched_pairs = pd.DataFrame(matches4, columns=[\"treated_index\", \"control_index\"])\n",
    "# Assign weights\n",
    "df[\"_weight\"] = 0.0\n",
    "df.loc[treated.index, \"_weight\"] = 1.0\n",
    "for treated_idx in matched_pairs[\"treated_index\"].unique():\n",
    "    controls = matched_pairs[matched_pairs[\"treated_index\"] == treated_idx][\"control_index\"]\n",
    "    df.loc[controls, \"_weight\"] += 1 / len(controls)\n",
    "df[\"weight\"] = df[\"_weight\"]\n",
    "# Replace zero weights with a small positive value\n",
    "df[\"weight\"] = df[\"weight\"].apply(lambda x: x if x > 0 else 1e-08)\n",
    "#df[\"weight\"] = df[\"weight\"] / df[\"weight\"].mean()\n",
    "# Save weights to file\n",
    "weights_path = f\"Weights_{datetime_string}.csv\"\n",
    "df[[\"ID\", \"weight\"]].to_csv(weights_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e1ecdd0-2f04-4579-8e37-09602eb34dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Standardized Mean Difference (SMD)\n",
    "def calculate_smd(treated, control, variable):\n",
    "    mean_treated = treated[variable].mean()\n",
    "    mean_control = control[variable].mean()\n",
    "    std_pooled = np.sqrt(\n",
    "        (treated[variable].var() + control[variable].var()) / 2\n",
    "    )\n",
    "    return (mean_treated - mean_control) / std_pooled\n",
    "\n",
    "# Define covariates for balance analysis\n",
    "covariates = [\"Education\", \"GenderHHead1women\", \"Age\", \"Disability\", \"NSize\", \"Anypersonolder60\", \"Chamber\"]\n",
    "\n",
    "# Separate treated and control groups before matching\n",
    "treated_before = df[df[\"T1\"] == 1]\n",
    "control_before = df[df[\"T1\"] == 0]\n",
    "\n",
    "# Subset matched pairs for after matching\n",
    "matched_control_indices = matched_pairs[\"control_index\"].values\n",
    "control_after = control_before.loc[matched_control_indices]\n",
    "treated_after = treated_before\n",
    "\n",
    "# Calculate SMD before and after matching\n",
    "smd_results = []\n",
    "for cov in covariates:\n",
    "    smd_before = calculate_smd(treated_before, control_before, cov)\n",
    "    smd_after = calculate_smd(treated_after, control_after, cov)\n",
    "    smd_results.append({\"Covariate\": cov, \"SMD Before\": smd_before, \"SMD After\": smd_after})\n",
    "\n",
    "# Convert to DataFrame for tabular output\n",
    "smd_df = pd.DataFrame(smd_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbe0791c-abb0-40bc-acfb-f6840761f235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Covariate  SMD Before  SMD After\n",
      "0          Education    0.161934   0.097025\n",
      "1  GenderHHead1women   -0.079849  -0.033056\n",
      "2                Age    0.291631   0.116805\n",
      "3         Disability    0.041452  -0.003107\n",
      "4              NSize   -0.271687   0.000036\n",
      "5   Anypersonolder60    0.304497   0.106261\n",
      "6            Chamber    0.212116   0.001474\n"
     ]
    }
   ],
   "source": [
    "# Display the balance table\n",
    "print(smd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a5d6b5-dbda-4473-9b72-13a5a52e7227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e894c0ae-b7d0-4565-b721-0915d5a3e5b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
