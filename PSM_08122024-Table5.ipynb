{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cf2625b-146a-4196-a11d-4c3a135d2af7",
   "metadata": {},
   "source": [
    "##  “Are there synergies and tradeoffs in sustainable heating from cleaner stoves and home insulation? Evidence from air pollution control policies in southern Chile” (Ref.: ENEECO-D-24-01813)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb9849-289f-49a2-bd76-4702001f46f0",
   "metadata": {},
   "source": [
    "###  Table 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a3c005-4ab9-4480-9ce9-dc96f0219fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from linearmodels.panel import PanelOLS\n",
    "# Set datetime string\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import pairwise_distances\n",
    "datetime_string = datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317b072b-ee35-48ac-964f-2684a6f79eab",
   "metadata": {},
   "source": [
    "####   Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72382f12-3c07-4de9-b35d-cb8e9319a560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "# Filter data\n",
    "df = df[df['tper'] < 2]\n",
    "# Define the variables to keep\n",
    "variables_to_keep = [\"ID\",\"T2\",\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[variables_to_keep]\n",
    "\n",
    "# Impute missing values with the median of each column\n",
    "columns_to_impute = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\"]\n",
    "for col in columns_to_impute:\n",
    "    median_value = df[col].median()  # Calculate the median\n",
    "    df[col] = df[col].fillna(median_value)  # Replace missing values with the median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ea075-1e76-40f2-a58e-bc29773a19c3",
   "metadata": {},
   "source": [
    "#### Function to extract results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29192180-0ce9-454a-a880-8657f8d23847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract T2ON results\n",
    "def extract_t2on_results(results, model_name):\n",
    "    \"\"\"\n",
    "    Extract T2ON parameter, standard error, t-stat, p-value, and confidence intervals from PanelOLS results.\n",
    "    \"\"\"\n",
    "    param = results.params[\"T2ON\"]\n",
    "    std_err = results.std_errors[\"T2ON\"]\n",
    "    t_stat = results.tstats[\"T2ON\"]\n",
    "    p_value = results.pvalues[\"T2ON\"]\n",
    "    ci_lower = param - 1.96 * std_err\n",
    "    ci_upper = param + 1.96 * std_err\n",
    "\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"INSULATION*ON Coefficient\": param,\n",
    "        \"Std. Error\": std_err,\n",
    "        \"T-stat\": t_stat,\n",
    "        \"P-value\": p_value,\n",
    "        \"CI Lower\": ci_lower,\n",
    "        \"CI Upper\": ci_upper,\n",
    "    }\n",
    "\n",
    "# Initialize a DataFrame to store results\n",
    "t2on_results_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe8c9fd4-06f3-423b-95ae-c3a06b005151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract T1T2ON results\n",
    "def extract_t1t2on_results(results, model_name):\n",
    "    \"\"\"\n",
    "    Extract T1T2ON parameter, standard error, t-stat, p-value, and confidence intervals from PanelOLS results.\n",
    "    \"\"\"\n",
    "    param = results.params[\"T1T2ON\"]\n",
    "    std_err = results.std_errors[\"T1T2ON\"]\n",
    "    t_stat = results.tstats[\"T1T2ON\"]\n",
    "    p_value = results.pvalues[\"T1T2ON\"]\n",
    "    ci_lower = param - 1.96 * std_err\n",
    "    ci_upper = param + 1.96 * std_err\n",
    "\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"PELLET*INSULATION*ON Coefficient\": param,\n",
    "        \"Std. Error\": std_err,\n",
    "        \"T-stat\": t_stat,\n",
    "        \"P-value\": p_value,\n",
    "        \"CI Lower\": ci_lower,\n",
    "        \"CI Upper\": ci_upper,\n",
    "    }\n",
    "\n",
    "# Initialize a DataFrame to store results\n",
    "t1t2on_results_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc62f38-29f4-4842-8dad-cb2f467264e8",
   "metadata": {},
   "source": [
    " #### Table 5. column 3.  The Effect of Pellet Stoves on PM2.5 Concentration (hourly observations)  - without matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1392c953-4f86-4d77-a2c9-d4cc7ff0e72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:                log_PMi   R-squared:                        0.4839\n",
      "Estimator:                   PanelOLS   R-squared (Between):              0.7620\n",
      "No. Observations:               17687   R-squared (Within):               0.4839\n",
      "Date:                Sun, Dec 08 2024   R-squared (Overall):              0.7593\n",
      "Time:                        02:09:02   Log-likelihood                   -2784.3\n",
      "Cov. Estimator:             Clustered                                           \n",
      "                                        F-statistic:                      2317.6\n",
      "Entities:                         379   P-value                           0.0000\n",
      "Avg Obs:                       46.668   Distribution:                 F(7,17301)\n",
      "Min Obs:                       7.0000                                           \n",
      "Max Obs:                       49.000   F-statistic (robust):             144.51\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                     865   Distribution:                 F(7,17301)\n",
      "Avg Obs:                       20.447                                           \n",
      "Min Obs:                       1.0000                                           \n",
      "Max Obs:                       35.000                                           \n",
      "                                                                                \n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "ON            -0.0054     0.0143    -0.3771     0.7061     -0.0334      0.0226\n",
      "T2ON           0.1096     0.0331     3.3104     0.0009      0.0447      0.1745\n",
      "T1T2ON        -0.1676     0.0356    -4.7041     0.0000     -0.2375     -0.0978\n",
      "log_PMo        0.5774     0.0202     28.608     0.0000      0.5378      0.6169\n",
      "log_To        -0.0087     0.0130    -0.6722     0.5015     -0.0342      0.0167\n",
      "DayExp        -0.0191     0.0076    -2.5047     0.0123     -0.0341     -0.0042\n",
      "Cycle         -0.0028     0.0045    -0.6109     0.5412     -0.0117      0.0061\n",
      "==============================================================================\n",
      "\n",
      "F-test for Poolability: 20.945\n",
      "P-value: 0.0000\n",
      "Distribution: F(378,17301)\n",
      "\n",
      "Included effects: Entity\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "relevant_vars = [\"ID\",\"IDHour\",\"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\",\"ON\", \"log_PMi\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[relevant_vars]\n",
    "df= df.dropna(subset=[\"log_PMi\",\"ON\", \"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"])\n",
    "# Save merged dataset\n",
    "panel_data_path = f\"PanelBDTemuco_{datetime_string}.csv\"\n",
    "df.to_csv(panel_data_path, index=False)\n",
    "# Prepare for panel regression\n",
    "df = df.sort_values(by=[\"ID\", \"IDHour\"])\n",
    "df = df.set_index([\"ID\", \"IDHour\"])\n",
    "\n",
    "from linearmodels.panel import PanelOLS\n",
    "# Define independent variables and dependent variable\n",
    "dependent_var = \"log_PMi\"\n",
    "independent_vars = [\"ON\", \"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"]\n",
    "# Fixed Effects Model\n",
    "model = PanelOLS.from_formula(\n",
    "    #f\"{dependent_var} ~ { ' + '.join(independent_vars)} + EntityEffects + TimeEffects\",\n",
    "    f\"{dependent_var} ~ { ' + '.join(independent_vars)} + EntityEffects\",\n",
    "    data=df,\n",
    "    drop_absorbed=True\n",
    ")\n",
    "results = model.fit(cov_type=\"clustered\", cluster_entity=True)\n",
    "\n",
    "# Extract T2ON results for the current model\n",
    "t2on_result = extract_t2on_results(results, model_name=\"model 1 - Without Matching\")\n",
    "t2on_results_df = pd.concat([t2on_results_df, pd.DataFrame([t2on_result])], ignore_index=True)\n",
    "\n",
    "# Extract T2ON results for the current model\n",
    "t1t2on_result = extract_t1t2on_results(results, model_name=\"model 1 - Without Matching\")\n",
    "t1t2on_results_df = pd.concat([t1t2on_results_df, pd.DataFrame([t1t2on_result])], ignore_index=True)\n",
    "\n",
    "print(results.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606c56f2-8669-46d9-96e9-02347c64570e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3e9b20d-6f00-4662-a056-5738e2779c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.654594\n",
      "         Iterations 5\n",
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:                log_PMi   R-squared:                        0.4865\n",
      "Estimator:                   PanelOLS   R-squared (Between):              0.7582\n",
      "No. Observations:               17687   R-squared (Within):               0.4865\n",
      "Date:                Sun, Dec 08 2024   R-squared (Overall):              0.7554\n",
      "Time:                        02:09:02   Log-likelihood                   -2623.1\n",
      "Cov. Estimator:             Clustered                                           \n",
      "                                        F-statistic:                      2341.5\n",
      "Entities:                         379   P-value                           0.0000\n",
      "Avg Obs:                       46.668   Distribution:                 F(7,17301)\n",
      "Min Obs:                       7.0000                                           \n",
      "Max Obs:                       49.000   F-statistic (robust):             129.86\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                     865   Distribution:                 F(7,17301)\n",
      "Avg Obs:                       20.447                                           \n",
      "Min Obs:                       1.0000                                           \n",
      "Max Obs:                       35.000                                           \n",
      "                                                                                \n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "ON             0.0094     0.0198     0.4722     0.6368     -0.0295      0.0482\n",
      "T2ON           0.0920     0.0354     2.6016     0.0093      0.0227      0.1614\n",
      "T1T2ON        -0.1674     0.0357    -4.6861     0.0000     -0.2374     -0.0974\n",
      "log_PMo        0.5817     0.0222     26.235     0.0000      0.5382      0.6252\n",
      "log_To        -0.0037     0.0140    -0.2611     0.7940     -0.0311      0.0238\n",
      "DayExp        -0.0212     0.0085    -2.4815     0.0131     -0.0379     -0.0045\n",
      "Cycle         -0.0035     0.0052    -0.6686     0.5038     -0.0136      0.0067\n",
      "==============================================================================\n",
      "\n",
      "F-test for Poolability: 21.625\n",
      "P-value: 0.0000\n",
      "Distribution: F(378,17301)\n",
      "\n",
      "Included effects: Entity\n"
     ]
    }
   ],
   "source": [
    "# Perform nearest neighbor matching\n",
    "neighbors = 2\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "# Filter data\n",
    "df = df[df['tper'] < 2]\n",
    "# Define the variables to keep\n",
    "variables_to_keep = [\"ID\",\"T2\",\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[variables_to_keep]\n",
    "\n",
    "# Impute missing values with the median of each column\n",
    "columns_to_impute = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\"]\n",
    "for col in columns_to_impute:\n",
    "    median_value = df[col].median()  # Calculate the median\n",
    "    df[col] = df[col].fillna(median_value)  # Replace missing values with the median\n",
    "# Logistic regression to estimate propensity scores\n",
    "covariates = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "X = sm.add_constant(df[covariates])\n",
    "y = df[\"T2\"]\n",
    "logit_model = sm.Logit(y, X).fit()\n",
    "df[\"logoddsT2\"] = logit_model.predict(X)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "logit_model = LogisticRegression(max_iter=500)\n",
    "df[\"propensity_score\"] = logit_model.fit(X_scaled, y).predict_proba(X_scaled)[:, 1]\n",
    "# Separate treated and control groups\n",
    "treated = df[df[\"T2\"] == 1]\n",
    "control = df[df[\"T2\"] == 0]\n",
    "\n",
    "# Apply caliper (0.01)\n",
    "caliper = 0.01\n",
    "matches = []\n",
    "nn = NearestNeighbors(n_neighbors=neighbors, metric=\"euclidean\")\n",
    "nn.fit(control[[\"propensity_score\"]])\n",
    "distances, indices = nn.kneighbors(treated[[\"propensity_score\"]])\n",
    "matches = []\n",
    "for i, dists in enumerate(distances):\n",
    "    matched_indices = indices[i][dists <= caliper]\n",
    "    if len(matched_indices) > 0:\n",
    "        for control_index in matched_indices:\n",
    "            matches.append((treated.index[i], control.iloc[control_index].name))\n",
    "# Generate matched DataFrame\n",
    "matched_pairs = pd.DataFrame(matches, columns=[\"treated_index\", \"control_index\"])\n",
    "# Assign weights\n",
    "df[\"_weight\"] = 0.0\n",
    "df.loc[treated.index, \"_weight\"] = 1.0\n",
    "for treated_idx in matched_pairs[\"treated_index\"].unique():\n",
    "    controls = matched_pairs[matched_pairs[\"treated_index\"] == treated_idx][\"control_index\"]\n",
    "    df.loc[controls, \"_weight\"] += 1 / len(controls)\n",
    "df[\"weight\"] = df[\"_weight\"]\n",
    "# Replace zero weights with a small positive value\n",
    "df[\"weight\"] = df[\"weight\"].apply(lambda x: x if x > 0 else 1e-08)\n",
    "#df[\"weight\"] = df[\"weight\"] / df[\"weight\"].mean()\n",
    "# Save weights to file\n",
    "weights_path = f\"Weights_{datetime_string}.csv\"\n",
    "df[[\"ID\", \"weight\"]].to_csv(weights_path, index=False)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "relevant_vars = [\"ID\",\"IDHour\",\"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\",\"ON\", \"log_PMi\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[relevant_vars]\n",
    "df= df.dropna(subset=[\"log_PMi\",\"ON\", \"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"])\n",
    "# Merge weights into original data\n",
    "weights_df = pd.read_csv(weights_path)\n",
    "df = pd.merge(df, weights_df, on=\"ID\", how=\"left\")\n",
    "# Save merged dataset\n",
    "panel_data_path = f\"PanelBDTemuco_{datetime_string}.csv\"\n",
    "df.to_csv(panel_data_path, index=False)\n",
    "# Prepare for panel regression\n",
    "df = df.sort_values(by=[\"ID\", \"IDHour\"])\n",
    "df = df.set_index([\"ID\", \"IDHour\"])\n",
    "\n",
    "model = PanelOLS.from_formula(\n",
    "    f\"{dependent_var} ~ { ' + '.join(independent_vars)} + EntityEffects\",\n",
    "    data=df,\n",
    "    weights=df[\"weight\"],\n",
    "    drop_absorbed=True\n",
    ")\n",
    "results = model.fit(cov_type=\"clustered\", cluster_entity=True)\n",
    "\n",
    "\n",
    "# Extract T2ON results for the current model\n",
    "t2on_result = extract_t2on_results(results, model_name=\"model 2, Nearest Neighbors Matching, n = 2\")\n",
    "t2on_results_df = pd.concat([t2on_results_df, pd.DataFrame([t2on_result])], ignore_index=True)\n",
    "\n",
    "# Extract T1T2ON results for the current model\n",
    "t1t2on_result = extract_t1t2on_results(results, model_name=\"model 2, Nearest Neighbors Matching, n = 2\")\n",
    "t1t2on_results_df = pd.concat([t1t2on_results_df, pd.DataFrame([t1t2on_result])], ignore_index=True)\n",
    "print(results.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c273b33f-3d94-438a-8ec4-0f52924c83ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.654594\n",
      "         Iterations 5\n",
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:                log_PMi   R-squared:                        0.4856\n",
      "Estimator:                   PanelOLS   R-squared (Between):              0.7549\n",
      "No. Observations:               17687   R-squared (Within):               0.4856\n",
      "Date:                Sun, Dec 08 2024   R-squared (Overall):              0.7522\n",
      "Time:                        02:09:02   Log-likelihood                   -2584.1\n",
      "Cov. Estimator:             Clustered                                           \n",
      "                                        F-statistic:                      2333.3\n",
      "Entities:                         379   P-value                           0.0000\n",
      "Avg Obs:                       46.668   Distribution:                 F(7,17301)\n",
      "Min Obs:                       7.0000                                           \n",
      "Max Obs:                       49.000   F-statistic (robust):             124.39\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                     865   Distribution:                 F(7,17301)\n",
      "Avg Obs:                       20.447                                           \n",
      "Min Obs:                       1.0000                                           \n",
      "Max Obs:                       35.000                                           \n",
      "                                                                                \n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "ON             0.0096     0.0198     0.4859     0.6270     -0.0292      0.0485\n",
      "T2ON           0.0936     0.0350     2.6730     0.0075      0.0250      0.1623\n",
      "T1T2ON        -0.1673     0.0357    -4.6871     0.0000     -0.2373     -0.0973\n",
      "log_PMo        0.5824     0.0226     25.734     0.0000      0.5380      0.6267\n",
      "log_To        -0.0089     0.0140    -0.6395     0.5225     -0.0364      0.0185\n",
      "DayExp        -0.0213     0.0081    -2.6273     0.0086     -0.0372     -0.0054\n",
      "Cycle         -0.0037     0.0051    -0.7202     0.4714     -0.0137      0.0063\n",
      "==============================================================================\n",
      "\n",
      "F-test for Poolability: 22.185\n",
      "P-value: 0.0000\n",
      "Distribution: F(378,17301)\n",
      "\n",
      "Included effects: Entity\n"
     ]
    }
   ],
   "source": [
    "# Perform nearest neighbor matching\n",
    "neighbors = 3\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "# Filter data\n",
    "df = df[df['tper'] < 2]\n",
    "# Define the variables to keep\n",
    "variables_to_keep = [\"ID\",\"T2\",\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[variables_to_keep]\n",
    "\n",
    "# Impute missing values with the median of each column\n",
    "columns_to_impute = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\"]\n",
    "for col in columns_to_impute:\n",
    "    median_value = df[col].median()  # Calculate the median\n",
    "    df[col] = df[col].fillna(median_value)  # Replace missing values with the median\n",
    "# Logistic regression to estimate propensity scores\n",
    "covariates = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "X = sm.add_constant(df[covariates])\n",
    "y = df[\"T2\"]\n",
    "logit_model = sm.Logit(y, X).fit()\n",
    "df[\"logoddsT2\"] = logit_model.predict(X)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "logit_model = LogisticRegression(max_iter=500)\n",
    "df[\"propensity_score\"] = logit_model.fit(X_scaled, y).predict_proba(X_scaled)[:, 1]\n",
    "# Separate treated and control groups\n",
    "treated = df[df[\"T2\"] == 1]\n",
    "control = df[df[\"T2\"] == 0]\n",
    "\n",
    "# Apply caliper (0.01)\n",
    "caliper = 0.01\n",
    "matches = []\n",
    "nn = NearestNeighbors(n_neighbors=neighbors, metric=\"euclidean\")\n",
    "nn.fit(control[[\"propensity_score\"]])\n",
    "distances, indices = nn.kneighbors(treated[[\"propensity_score\"]])\n",
    "matches = []\n",
    "for i, dists in enumerate(distances):\n",
    "    matched_indices = indices[i][dists <= caliper]\n",
    "    if len(matched_indices) > 0:\n",
    "        for control_index in matched_indices:\n",
    "            matches.append((treated.index[i], control.iloc[control_index].name))\n",
    "# Generate matched DataFrame\n",
    "matched_pairs = pd.DataFrame(matches, columns=[\"treated_index\", \"control_index\"])\n",
    "# Assign weights\n",
    "df[\"_weight\"] = 0.0\n",
    "df.loc[treated.index, \"_weight\"] = 1.0\n",
    "for treated_idx in matched_pairs[\"treated_index\"].unique():\n",
    "    controls = matched_pairs[matched_pairs[\"treated_index\"] == treated_idx][\"control_index\"]\n",
    "    df.loc[controls, \"_weight\"] += 1 / len(controls)\n",
    "df[\"weight\"] = df[\"_weight\"]\n",
    "# Replace zero weights with a small positive value\n",
    "df[\"weight\"] = df[\"weight\"].apply(lambda x: x if x > 0 else 1e-08)\n",
    "#df[\"weight\"] = df[\"weight\"] / df[\"weight\"].mean()\n",
    "# Save weights to file\n",
    "weights_path = f\"Weights_{datetime_string}.csv\"\n",
    "df[[\"ID\", \"weight\"]].to_csv(weights_path, index=False)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "relevant_vars = [\"ID\",\"IDHour\",\"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\",\"ON\", \"log_PMi\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[relevant_vars]\n",
    "df= df.dropna(subset=[\"log_PMi\",\"ON\", \"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"])\n",
    "# Merge weights into original data\n",
    "weights_df = pd.read_csv(weights_path)\n",
    "df = pd.merge(df, weights_df, on=\"ID\", how=\"left\")\n",
    "# Save merged dataset\n",
    "panel_data_path = f\"PanelBDTemuco_{datetime_string}.csv\"\n",
    "df.to_csv(panel_data_path, index=False)\n",
    "# Prepare for panel regression\n",
    "df = df.sort_values(by=[\"ID\", \"IDHour\"])\n",
    "df = df.set_index([\"ID\", \"IDHour\"])\n",
    "\n",
    "model = PanelOLS.from_formula(\n",
    "    f\"{dependent_var} ~ { ' + '.join(independent_vars)} + EntityEffects\",\n",
    "    data=df,\n",
    "    weights=df[\"weight\"],\n",
    "    drop_absorbed=True\n",
    ")\n",
    "results = model.fit(cov_type=\"clustered\", cluster_entity=True)\n",
    "\n",
    "\n",
    "# Extract T2ON results for the current model\n",
    "t2on_result = extract_t2on_results(results, model_name=\"model 3, Nearest Neighbors Matching, n = 3\")\n",
    "t2on_results_df = pd.concat([t2on_results_df, pd.DataFrame([t2on_result])], ignore_index=True)\n",
    "\n",
    "# Extract T1T2ON results for the current model\n",
    "t1t2on_result = extract_t1t2on_results(results, model_name=\"model 3, Nearest Neighbors Matching, n = 3\")\n",
    "t1t2on_results_df = pd.concat([t1t2on_results_df, pd.DataFrame([t1t2on_result])], ignore_index=True)\n",
    "print(results.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9c3a702-6905-4a09-8bc2-146861399a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.654594\n",
      "         Iterations 5\n",
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:                log_PMi   R-squared:                        0.4867\n",
      "Estimator:                   PanelOLS   R-squared (Between):              0.7535\n",
      "No. Observations:               17687   R-squared (Within):               0.4867\n",
      "Date:                Sun, Dec 08 2024   R-squared (Overall):              0.7508\n",
      "Time:                        02:09:02   Log-likelihood                   -2668.2\n",
      "Cov. Estimator:             Clustered                                           \n",
      "                                        F-statistic:                      2343.9\n",
      "Entities:                         379   P-value                           0.0000\n",
      "Avg Obs:                       46.668   Distribution:                 F(7,17301)\n",
      "Min Obs:                       7.0000                                           \n",
      "Max Obs:                       49.000   F-statistic (robust):             131.97\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                     865   Distribution:                 F(7,17301)\n",
      "Avg Obs:                       20.447                                           \n",
      "Min Obs:                       1.0000                                           \n",
      "Max Obs:                       35.000                                           \n",
      "                                                                                \n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "ON             0.0081     0.0184     0.4403     0.6597     -0.0279      0.0441\n",
      "T2ON           0.0956     0.0345     2.7739     0.0055      0.0281      0.1632\n",
      "T1T2ON        -0.1673     0.0357    -4.6870     0.0000     -0.2372     -0.0973\n",
      "log_PMo        0.5815     0.0216     26.912     0.0000      0.5391      0.6238\n",
      "log_To        -0.0094     0.0138    -0.6776     0.4980     -0.0365      0.0178\n",
      "DayExp        -0.0213     0.0083    -2.5743     0.0101     -0.0376     -0.0051\n",
      "Cycle         -0.0039     0.0051    -0.7545     0.4505     -0.0139      0.0062\n",
      "==============================================================================\n",
      "\n",
      "F-test for Poolability: 21.329\n",
      "P-value: 0.0000\n",
      "Distribution: F(378,17301)\n",
      "\n",
      "Included effects: Entity\n"
     ]
    }
   ],
   "source": [
    "# Perform nearest neighbor matching\n",
    "neighbors = 4\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "# Filter data\n",
    "df = df[df['tper'] < 2]\n",
    "# Define the variables to keep\n",
    "variables_to_keep = [\"ID\",\"T2\",\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[variables_to_keep]\n",
    "\n",
    "# Impute missing values with the median of each column\n",
    "columns_to_impute = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\"]\n",
    "for col in columns_to_impute:\n",
    "    median_value = df[col].median()  # Calculate the median\n",
    "    df[col] = df[col].fillna(median_value)  # Replace missing values with the median\n",
    "# Logistic regression to estimate propensity scores\n",
    "covariates = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "X = sm.add_constant(df[covariates])\n",
    "y = df[\"T2\"]\n",
    "logit_model = sm.Logit(y, X).fit()\n",
    "df[\"logoddsT2\"] = logit_model.predict(X)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "logit_model = LogisticRegression(max_iter=500)\n",
    "df[\"propensity_score\"] = logit_model.fit(X_scaled, y).predict_proba(X_scaled)[:, 1]\n",
    "# Separate treated and control groups\n",
    "treated = df[df[\"T2\"] == 1]\n",
    "control = df[df[\"T2\"] == 0]\n",
    "\n",
    "# Apply caliper (0.01)\n",
    "caliper = 0.01\n",
    "matches = []\n",
    "nn = NearestNeighbors(n_neighbors=neighbors, metric=\"euclidean\")\n",
    "nn.fit(control[[\"propensity_score\"]])\n",
    "distances, indices = nn.kneighbors(treated[[\"propensity_score\"]])\n",
    "matches = []\n",
    "for i, dists in enumerate(distances):\n",
    "    matched_indices = indices[i][dists <= caliper]\n",
    "    if len(matched_indices) > 0:\n",
    "        for control_index in matched_indices:\n",
    "            matches.append((treated.index[i], control.iloc[control_index].name))\n",
    "# Generate matched DataFrame\n",
    "matched_pairs = pd.DataFrame(matches, columns=[\"treated_index\", \"control_index\"])\n",
    "# Assign weights\n",
    "df[\"_weight\"] = 0.0\n",
    "df.loc[treated.index, \"_weight\"] = 1.0\n",
    "for treated_idx in matched_pairs[\"treated_index\"].unique():\n",
    "    controls = matched_pairs[matched_pairs[\"treated_index\"] == treated_idx][\"control_index\"]\n",
    "    df.loc[controls, \"_weight\"] += 1 / len(controls)\n",
    "df[\"weight\"] = df[\"_weight\"]\n",
    "# Replace zero weights with a small positive value\n",
    "df[\"weight\"] = df[\"weight\"].apply(lambda x: x if x > 0 else 1e-08)\n",
    "#df[\"weight\"] = df[\"weight\"] / df[\"weight\"].mean()\n",
    "# Save weights to file\n",
    "weights_path = f\"Weights_{datetime_string}.csv\"\n",
    "df[[\"ID\", \"weight\"]].to_csv(weights_path, index=False)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "relevant_vars = [\"ID\",\"IDHour\",\"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\",\"ON\", \"log_PMi\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[relevant_vars]\n",
    "df= df.dropna(subset=[\"log_PMi\",\"ON\", \"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"])\n",
    "# Merge weights into original data\n",
    "weights_df = pd.read_csv(weights_path)\n",
    "df = pd.merge(df, weights_df, on=\"ID\", how=\"left\")\n",
    "# Save merged dataset\n",
    "panel_data_path = f\"PanelBDTemuco_{datetime_string}.csv\"\n",
    "df.to_csv(panel_data_path, index=False)\n",
    "# Prepare for panel regression\n",
    "df = df.sort_values(by=[\"ID\", \"IDHour\"])\n",
    "df = df.set_index([\"ID\", \"IDHour\"])\n",
    "\n",
    "model = PanelOLS.from_formula(\n",
    "    f\"{dependent_var} ~ { ' + '.join(independent_vars)} + EntityEffects\",\n",
    "    data=df,\n",
    "    weights=df[\"weight\"],\n",
    "    drop_absorbed=True\n",
    ")\n",
    "results = model.fit(cov_type=\"clustered\", cluster_entity=True)\n",
    "\n",
    "\n",
    "# Extract T2ON results for the current model\n",
    "t2on_result = extract_t2on_results(results, model_name=\"model 4, Nearest Neighbors Matching, n = 4\")\n",
    "t2on_results_df = pd.concat([t2on_results_df, pd.DataFrame([t2on_result])], ignore_index=True)\n",
    "\n",
    "# Extract T1T2ON results for the current model\n",
    "t1t2on_result = extract_t1t2on_results(results, model_name=\"model 4, Nearest Neighbors Matching, n = 4\")\n",
    "t1t2on_results_df = pd.concat([t1t2on_results_df, pd.DataFrame([t1t2on_result])], ignore_index=True)\n",
    "print(results.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e23766d-f839-450c-8303-43fc7290ee28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.654594\n",
      "         Iterations 5\n",
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:                log_PMi   R-squared:                        0.4869\n",
      "Estimator:                   PanelOLS   R-squared (Between):              0.7564\n",
      "No. Observations:               17687   R-squared (Within):               0.4869\n",
      "Date:                Sun, Dec 08 2024   R-squared (Overall):              0.7536\n",
      "Time:                        02:09:03   Log-likelihood                   -2671.6\n",
      "Cov. Estimator:             Clustered                                           \n",
      "                                        F-statistic:                      2345.1\n",
      "Entities:                         379   P-value                           0.0000\n",
      "Avg Obs:                       46.668   Distribution:                 F(7,17301)\n",
      "Min Obs:                       7.0000                                           \n",
      "Max Obs:                       49.000   F-statistic (robust):             133.74\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                     865   Distribution:                 F(7,17301)\n",
      "Avg Obs:                       20.447                                           \n",
      "Min Obs:                       1.0000                                           \n",
      "Max Obs:                       35.000                                           \n",
      "                                                                                \n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "ON             0.0067     0.0187     0.3552     0.7225     -0.0301      0.0434\n",
      "T2ON           0.0972     0.0347     2.8023     0.0051      0.0292      0.1652\n",
      "T1T2ON        -0.1674     0.0357    -4.6917     0.0000     -0.2374     -0.0975\n",
      "log_PMo        0.5812     0.0214     27.135     0.0000      0.5392      0.6232\n",
      "log_To        -0.0099     0.0139    -0.7141     0.4752     -0.0371      0.0173\n",
      "DayExp        -0.0207     0.0083    -2.5017     0.0124     -0.0369     -0.0045\n",
      "Cycle         -0.0034     0.0051    -0.6708     0.5024     -0.0134      0.0066\n",
      "==============================================================================\n",
      "\n",
      "F-test for Poolability: 21.442\n",
      "P-value: 0.0000\n",
      "Distribution: F(378,17301)\n",
      "\n",
      "Included effects: Entity\n"
     ]
    }
   ],
   "source": [
    "# Perform nearest neighbor matching\n",
    "neighbors = 5\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "# Filter data\n",
    "df = df[df['tper'] < 2]\n",
    "# Define the variables to keep\n",
    "variables_to_keep = [\"ID\",\"T2\",\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[variables_to_keep]\n",
    "\n",
    "# Impute missing values with the median of each column\n",
    "columns_to_impute = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\"]\n",
    "for col in columns_to_impute:\n",
    "    median_value = df[col].median()  # Calculate the median\n",
    "    df[col] = df[col].fillna(median_value)  # Replace missing values with the median\n",
    "# Logistic regression to estimate propensity scores\n",
    "covariates = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "X = sm.add_constant(df[covariates])\n",
    "y = df[\"T2\"]\n",
    "logit_model = sm.Logit(y, X).fit()\n",
    "df[\"logoddsT2\"] = logit_model.predict(X)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "logit_model = LogisticRegression(max_iter=500)\n",
    "df[\"propensity_score\"] = logit_model.fit(X_scaled, y).predict_proba(X_scaled)[:, 1]\n",
    "# Separate treated and control groups\n",
    "treated = df[df[\"T2\"] == 1]\n",
    "control = df[df[\"T2\"] == 0]\n",
    "\n",
    "# Apply caliper (0.01)\n",
    "caliper = 0.01\n",
    "matches = []\n",
    "nn = NearestNeighbors(n_neighbors=neighbors, metric=\"euclidean\")\n",
    "nn.fit(control[[\"propensity_score\"]])\n",
    "distances, indices = nn.kneighbors(treated[[\"propensity_score\"]])\n",
    "matches = []\n",
    "for i, dists in enumerate(distances):\n",
    "    matched_indices = indices[i][dists <= caliper]\n",
    "    if len(matched_indices) > 0:\n",
    "        for control_index in matched_indices:\n",
    "            matches.append((treated.index[i], control.iloc[control_index].name))\n",
    "# Generate matched DataFrame\n",
    "matched_pairs = pd.DataFrame(matches, columns=[\"treated_index\", \"control_index\"])\n",
    "# Assign weights\n",
    "df[\"_weight\"] = 0.0\n",
    "df.loc[treated.index, \"_weight\"] = 1.0\n",
    "for treated_idx in matched_pairs[\"treated_index\"].unique():\n",
    "    controls = matched_pairs[matched_pairs[\"treated_index\"] == treated_idx][\"control_index\"]\n",
    "    df.loc[controls, \"_weight\"] += 1 / len(controls)\n",
    "df[\"weight\"] = df[\"_weight\"]\n",
    "# Replace zero weights with a small positive value\n",
    "df[\"weight\"] = df[\"weight\"].apply(lambda x: x if x > 0 else 1e-08)\n",
    "#df[\"weight\"] = df[\"weight\"] / df[\"weight\"].mean()\n",
    "# Save weights to file\n",
    "weights_path = f\"Weights_{datetime_string}.csv\"\n",
    "df[[\"ID\", \"weight\"]].to_csv(weights_path, index=False)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "relevant_vars = [\"ID\",\"IDHour\",\"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\",\"ON\", \"log_PMi\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[relevant_vars]\n",
    "df= df.dropna(subset=[\"log_PMi\",\"ON\", \"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"])\n",
    "# Merge weights into original data\n",
    "weights_df = pd.read_csv(weights_path)\n",
    "df = pd.merge(df, weights_df, on=\"ID\", how=\"left\")\n",
    "# Save merged dataset\n",
    "panel_data_path = f\"PanelBDTemuco_{datetime_string}.csv\"\n",
    "df.to_csv(panel_data_path, index=False)\n",
    "# Prepare for panel regression\n",
    "df = df.sort_values(by=[\"ID\", \"IDHour\"])\n",
    "df = df.set_index([\"ID\", \"IDHour\"])\n",
    "\n",
    "model = PanelOLS.from_formula(\n",
    "    f\"{dependent_var} ~ { ' + '.join(independent_vars)} + EntityEffects\",\n",
    "    data=df,\n",
    "    weights=df[\"weight\"],\n",
    "    drop_absorbed=True\n",
    ")\n",
    "results = model.fit(cov_type=\"clustered\", cluster_entity=True)\n",
    "\n",
    "\n",
    "# Extract T2ON results for the current model\n",
    "t2on_result = extract_t2on_results(results, model_name=\"model 5, Nearest Neighbors Matching, n = 5\")\n",
    "t2on_results_df = pd.concat([t2on_results_df, pd.DataFrame([t2on_result])], ignore_index=True)\n",
    "\n",
    "# Extract T1T2ON results for the current model\n",
    "t1t2on_result = extract_t1t2on_results(results, model_name=\"model 5, Nearest Neighbors Matching, n = 5\")\n",
    "t1t2on_results_df = pd.concat([t1t2on_results_df, pd.DataFrame([t1t2on_result])], ignore_index=True)\n",
    "print(results.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "766b8ce5-3dee-4906-9473-640dd047de68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.654594\n",
      "         Iterations 5\n",
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:                log_PMi   R-squared:                        0.4874\n",
      "Estimator:                   PanelOLS   R-squared (Between):              0.7536\n",
      "No. Observations:               17687   R-squared (Within):               0.4874\n",
      "Date:                Sun, Dec 08 2024   R-squared (Overall):              0.7508\n",
      "Time:                        02:09:03   Log-likelihood                   -2683.2\n",
      "Cov. Estimator:             Clustered                                           \n",
      "                                        F-statistic:                      2349.8\n",
      "Entities:                         379   P-value                           0.0000\n",
      "Avg Obs:                       46.668   Distribution:                 F(7,17301)\n",
      "Min Obs:                       7.0000                                           \n",
      "Max Obs:                       49.000   F-statistic (robust):             134.69\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                     865   Distribution:                 F(7,17301)\n",
      "Avg Obs:                       20.447                                           \n",
      "Min Obs:                       1.0000                                           \n",
      "Max Obs:                       35.000                                           \n",
      "                                                                                \n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "ON             0.0074     0.0187     0.3929     0.6944     -0.0293      0.0440\n",
      "T2ON           0.0968     0.0347     2.7914     0.0053      0.0288      0.1648\n",
      "T1T2ON        -0.1672     0.0357    -4.6856     0.0000     -0.2372     -0.0973\n",
      "log_PMo        0.5822     0.0214     27.241     0.0000      0.5403      0.6241\n",
      "log_To        -0.0106     0.0140    -0.7618     0.4462     -0.0380      0.0167\n",
      "DayExp        -0.0213     0.0083    -2.5742     0.0101     -0.0375     -0.0051\n",
      "Cycle         -0.0040     0.0051    -0.7875     0.4310     -0.0139      0.0059\n",
      "==============================================================================\n",
      "\n",
      "F-test for Poolability: 21.336\n",
      "P-value: 0.0000\n",
      "Distribution: F(378,17301)\n",
      "\n",
      "Included effects: Entity\n"
     ]
    }
   ],
   "source": [
    "# Perform nearest neighbor matching\n",
    "neighbors = 6\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "# Filter data\n",
    "df = df[df['tper'] < 2]\n",
    "# Define the variables to keep\n",
    "variables_to_keep = [\"ID\",\"T2\",\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[variables_to_keep]\n",
    "\n",
    "# Impute missing values with the median of each column\n",
    "columns_to_impute = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\"]\n",
    "for col in columns_to_impute:\n",
    "    median_value = df[col].median()  # Calculate the median\n",
    "    df[col] = df[col].fillna(median_value)  # Replace missing values with the median\n",
    "# Logistic regression to estimate propensity scores\n",
    "covariates = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "X = sm.add_constant(df[covariates])\n",
    "y = df[\"T2\"]\n",
    "logit_model = sm.Logit(y, X).fit()\n",
    "df[\"logoddsT2\"] = logit_model.predict(X)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "logit_model = LogisticRegression(max_iter=500)\n",
    "df[\"propensity_score\"] = logit_model.fit(X_scaled, y).predict_proba(X_scaled)[:, 1]\n",
    "# Separate treated and control groups\n",
    "treated = df[df[\"T2\"] == 1]\n",
    "control = df[df[\"T2\"] == 0]\n",
    "\n",
    "# Apply caliper (0.01)\n",
    "caliper = 0.01\n",
    "matches = []\n",
    "nn = NearestNeighbors(n_neighbors=neighbors, metric=\"euclidean\")\n",
    "nn.fit(control[[\"propensity_score\"]])\n",
    "distances, indices = nn.kneighbors(treated[[\"propensity_score\"]])\n",
    "matches = []\n",
    "for i, dists in enumerate(distances):\n",
    "    matched_indices = indices[i][dists <= caliper]\n",
    "    if len(matched_indices) > 0:\n",
    "        for control_index in matched_indices:\n",
    "            matches.append((treated.index[i], control.iloc[control_index].name))\n",
    "# Generate matched DataFrame\n",
    "matched_pairs = pd.DataFrame(matches, columns=[\"treated_index\", \"control_index\"])\n",
    "# Assign weights\n",
    "df[\"_weight\"] = 0.0\n",
    "df.loc[treated.index, \"_weight\"] = 1.0\n",
    "for treated_idx in matched_pairs[\"treated_index\"].unique():\n",
    "    controls = matched_pairs[matched_pairs[\"treated_index\"] == treated_idx][\"control_index\"]\n",
    "    df.loc[controls, \"_weight\"] += 1 / len(controls)\n",
    "df[\"weight\"] = df[\"_weight\"]\n",
    "# Replace zero weights with a small positive value\n",
    "df[\"weight\"] = df[\"weight\"].apply(lambda x: x if x > 0 else 1e-08)\n",
    "#df[\"weight\"] = df[\"weight\"] / df[\"weight\"].mean()\n",
    "# Save weights to file\n",
    "weights_path = f\"Weights_{datetime_string}.csv\"\n",
    "df[[\"ID\", \"weight\"]].to_csv(weights_path, index=False)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "relevant_vars = [\"ID\",\"IDHour\",\"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\",\"ON\", \"log_PMi\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[relevant_vars]\n",
    "df= df.dropna(subset=[\"log_PMi\",\"ON\", \"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"])\n",
    "# Merge weights into original data\n",
    "weights_df = pd.read_csv(weights_path)\n",
    "df = pd.merge(df, weights_df, on=\"ID\", how=\"left\")\n",
    "# Save merged dataset\n",
    "panel_data_path = f\"PanelBDTemuco_{datetime_string}.csv\"\n",
    "df.to_csv(panel_data_path, index=False)\n",
    "# Prepare for panel regression\n",
    "df = df.sort_values(by=[\"ID\", \"IDHour\"])\n",
    "df = df.set_index([\"ID\", \"IDHour\"])\n",
    "\n",
    "model = PanelOLS.from_formula(\n",
    "    f\"{dependent_var} ~ { ' + '.join(independent_vars)} + EntityEffects\",\n",
    "    data=df,\n",
    "    weights=df[\"weight\"],\n",
    "    drop_absorbed=True\n",
    ")\n",
    "results = model.fit(cov_type=\"clustered\", cluster_entity=True)\n",
    "\n",
    "\n",
    "# Extract T2ON results for the current model\n",
    "t2on_result = extract_t2on_results(results, model_name=\"model 6, Nearest Neighbors Matching, n = 6\")\n",
    "t2on_results_df = pd.concat([t2on_results_df, pd.DataFrame([t2on_result])], ignore_index=True)\n",
    "\n",
    "# Extract T1T2ON results for the current model\n",
    "t1t2on_result = extract_t1t2on_results(results, model_name=\"model 6, Nearest Neighbors Matching, n = 6\")\n",
    "t1t2on_results_df = pd.concat([t1t2on_results_df, pd.DataFrame([t1t2on_result])], ignore_index=True)\n",
    "print(results.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6cc00e-20fa-4359-8f11-96baca3faf6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0167223-3305-4c75-b74e-2a0ddeb3f90d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>INSULATION*ON Coefficient</th>\n",
       "      <th>Std. Error</th>\n",
       "      <th>T-stat</th>\n",
       "      <th>P-value</th>\n",
       "      <th>CI Lower</th>\n",
       "      <th>CI Upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model 1 - Without Matching</td>\n",
       "      <td>0.109599</td>\n",
       "      <td>0.033108</td>\n",
       "      <td>3.310386</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.044708</td>\n",
       "      <td>0.174490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model 2, Nearest Neighbors Matching, n = 2</td>\n",
       "      <td>0.092023</td>\n",
       "      <td>0.035372</td>\n",
       "      <td>2.601591</td>\n",
       "      <td>0.009287</td>\n",
       "      <td>0.022694</td>\n",
       "      <td>0.161352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model 3, Nearest Neighbors Matching, n = 3</td>\n",
       "      <td>0.093646</td>\n",
       "      <td>0.035034</td>\n",
       "      <td>2.672977</td>\n",
       "      <td>0.007525</td>\n",
       "      <td>0.024979</td>\n",
       "      <td>0.162313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model 4, Nearest Neighbors Matching, n = 4</td>\n",
       "      <td>0.095632</td>\n",
       "      <td>0.034476</td>\n",
       "      <td>2.773893</td>\n",
       "      <td>0.005545</td>\n",
       "      <td>0.028060</td>\n",
       "      <td>0.163205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model 5, Nearest Neighbors Matching, n = 5</td>\n",
       "      <td>0.097205</td>\n",
       "      <td>0.034687</td>\n",
       "      <td>2.802342</td>\n",
       "      <td>0.005079</td>\n",
       "      <td>0.029218</td>\n",
       "      <td>0.165191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>model 6, Nearest Neighbors Matching, n = 6</td>\n",
       "      <td>0.096841</td>\n",
       "      <td>0.034693</td>\n",
       "      <td>2.791368</td>\n",
       "      <td>0.005254</td>\n",
       "      <td>0.028843</td>\n",
       "      <td>0.164839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Model  INSULATION*ON Coefficient  \\\n",
       "0                  model 1 - Without Matching                   0.109599   \n",
       "1  model 2, Nearest Neighbors Matching, n = 2                   0.092023   \n",
       "2  model 3, Nearest Neighbors Matching, n = 3                   0.093646   \n",
       "3  model 4, Nearest Neighbors Matching, n = 4                   0.095632   \n",
       "4  model 5, Nearest Neighbors Matching, n = 5                   0.097205   \n",
       "5  model 6, Nearest Neighbors Matching, n = 6                   0.096841   \n",
       "\n",
       "   Std. Error    T-stat   P-value  CI Lower  CI Upper  \n",
       "0    0.033108  3.310386  0.000934  0.044708  0.174490  \n",
       "1    0.035372  2.601591  0.009287  0.022694  0.161352  \n",
       "2    0.035034  2.672977  0.007525  0.024979  0.162313  \n",
       "3    0.034476  2.773893  0.005545  0.028060  0.163205  \n",
       "4    0.034687  2.802342  0.005079  0.029218  0.165191  \n",
       "5    0.034693  2.791368  0.005254  0.028843  0.164839  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2on_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "385b4f22-c166-4f41-bbdc-5420cfdc5e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>PELLET*INSULATION*ON Coefficient</th>\n",
       "      <th>Std. Error</th>\n",
       "      <th>T-stat</th>\n",
       "      <th>P-value</th>\n",
       "      <th>CI Lower</th>\n",
       "      <th>CI Upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model 1 - Without Matching</td>\n",
       "      <td>-0.167646</td>\n",
       "      <td>0.035638</td>\n",
       "      <td>-4.704128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.237496</td>\n",
       "      <td>-0.097795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model 2, Nearest Neighbors Matching, n = 2</td>\n",
       "      <td>-0.167365</td>\n",
       "      <td>0.035715</td>\n",
       "      <td>-4.686112</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.237367</td>\n",
       "      <td>-0.097363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model 3, Nearest Neighbors Matching, n = 3</td>\n",
       "      <td>-0.167322</td>\n",
       "      <td>0.035698</td>\n",
       "      <td>-4.687084</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.237291</td>\n",
       "      <td>-0.097353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model 4, Nearest Neighbors Matching, n = 4</td>\n",
       "      <td>-0.167273</td>\n",
       "      <td>0.035689</td>\n",
       "      <td>-4.687018</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.237223</td>\n",
       "      <td>-0.097323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model 5, Nearest Neighbors Matching, n = 5</td>\n",
       "      <td>-0.167417</td>\n",
       "      <td>0.035683</td>\n",
       "      <td>-4.691743</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.237356</td>\n",
       "      <td>-0.097478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>model 6, Nearest Neighbors Matching, n = 6</td>\n",
       "      <td>-0.167243</td>\n",
       "      <td>0.035693</td>\n",
       "      <td>-4.685610</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.237201</td>\n",
       "      <td>-0.097285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Model  \\\n",
       "0                  model 1 - Without Matching   \n",
       "1  model 2, Nearest Neighbors Matching, n = 2   \n",
       "2  model 3, Nearest Neighbors Matching, n = 3   \n",
       "3  model 4, Nearest Neighbors Matching, n = 4   \n",
       "4  model 5, Nearest Neighbors Matching, n = 5   \n",
       "5  model 6, Nearest Neighbors Matching, n = 6   \n",
       "\n",
       "   PELLET*INSULATION*ON Coefficient  Std. Error    T-stat   P-value  CI Lower  \\\n",
       "0                         -0.167646    0.035638 -4.704128  0.000003 -0.237496   \n",
       "1                         -0.167365    0.035715 -4.686112  0.000003 -0.237367   \n",
       "2                         -0.167322    0.035698 -4.687084  0.000003 -0.237291   \n",
       "3                         -0.167273    0.035689 -4.687018  0.000003 -0.237223   \n",
       "4                         -0.167417    0.035683 -4.691743  0.000003 -0.237356   \n",
       "5                         -0.167243    0.035693 -4.685610  0.000003 -0.237201   \n",
       "\n",
       "   CI Upper  \n",
       "0 -0.097795  \n",
       "1 -0.097363  \n",
       "2 -0.097353  \n",
       "3 -0.097323  \n",
       "4 -0.097478  \n",
       "5 -0.097285  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1t2on_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb2980e-133a-40d0-8780-b6984a3e4810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acdb41a5-6813-42c8-b8fb-f1e8ce7e18bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "# Filter data\n",
    "df = df[df['tper'] < 2]\n",
    "variables_to_keep = [\"ID\",\"T2\",\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "df = df[variables_to_keep]\n",
    "# Impute missing values with the median\n",
    "columns_to_impute = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\"]\n",
    "for col in columns_to_impute:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "# Logistic regression to estimate propensity scores\n",
    "covariates = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "X = df[covariates]\n",
    "y = df[\"T2\"]\n",
    "# Standardize covariates\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "logit_model = LogisticRegression(max_iter=500)\n",
    "df[\"propensity_score\"] = logit_model.fit(X_scaled, y).predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "# Separate treated and control groups\n",
    "treated = df[df[\"T2\"] == 1]\n",
    "control = df[df[\"T2\"] == 0]\n",
    "\n",
    "# Perform kernel matching using Gaussian kernel\n",
    "bandwidth = 0.1  # Adjust bandwidth as needed\n",
    "distances = pairwise_distances(treated[[\"propensity_score\"]], control[[\"propensity_score\"]], metric=\"euclidean\")\n",
    "kernel_weights = np.exp(-distances**2 / (2 * bandwidth**2))\n",
    "# Assign weights for control units\n",
    "df[\"_weight\"] = 0.0\n",
    "df.loc[treated.index, \"_weight\"] = 1.0  # Full weight for treated units\n",
    "for i, treated_idx in enumerate(treated.index):\n",
    "    for j, control_idx in enumerate(control.index):\n",
    "        df.loc[control_idx, \"_weight\"] += kernel_weights[i, j]\n",
    "df[\"weight\"] = df[\"_weight\"]\n",
    "# Replace zero weights with a small positive value\n",
    "df[\"weight\"] = df[\"weight\"].apply(lambda x: x if x > 0 else 1e-08)\n",
    "# Save weights to a file\n",
    "weights_path = f\"Weights_{datetime_string}.csv\"\n",
    "df[[\"ID\", \"weight\"]].to_csv(weights_path, index=False)\n",
    "# Load the original dataset for panel regression\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "relevant_vars = [\"ID\",\"IDHour\",\"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\",\"ON\", \"log_PMi\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[relevant_vars]\n",
    "df= df.dropna(subset=[\"log_PMi\",\"ON\", \"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"])\n",
    "# Merge weights into original data\n",
    "weights_df = pd.read_csv(weights_path)\n",
    "df = pd.merge(df, weights_df, on=\"ID\", how=\"left\")\n",
    "# Save merged dataset\n",
    "panel_data_path = f\"PanelBDTemuco_{datetime_string}.csv\"\n",
    "df.to_csv(panel_data_path, index=False)\n",
    "# Prepare for panel regression\n",
    "df = df.sort_values(by=[\"ID\", \"IDHour\"])\n",
    "df = df.set_index([\"ID\", \"IDHour\"])\n",
    "# Fixed Effects Model without weights\n",
    "dependent_var = \"log_PMi\"\n",
    "independent_vars = [\"ON\", \"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"]\n",
    "\n",
    "# Fixed Effects Model with weights\n",
    "model = PanelOLS.from_formula(\n",
    "    f\"{dependent_var} ~ { ' + '.join(independent_vars)} + EntityEffects\",\n",
    "    data=df,\n",
    "    weights=df[\"weight\"],\n",
    "    drop_absorbed=True\n",
    ")\n",
    "results = model.fit(cov_type=\"clustered\", cluster_entity=True)\n",
    "# Extract T2ON results for the current model\n",
    "t2on_result = extract_t2on_results(results, model_name=\"model Kernel Matching, bandwidth = 0.1\")\n",
    "t2on_results_df = pd.concat([t2on_results_df, pd.DataFrame([t2on_result])], ignore_index=True)\n",
    "\n",
    "# Extract T1T2ON results for the current model\n",
    "t1t2on_result = extract_t1t2on_results(results, model_name=\"model Kernel Matching, bandwidth = 0.1\")\n",
    "t1t2on_results_df = pd.concat([t1t2on_results_df, pd.DataFrame([t1t2on_result])], ignore_index=True)\n",
    "#print(results.summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6070a6cf-6f7d-48b4-baa4-7482338f2465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "# Filter data\n",
    "df = df[df['tper'] < 2]\n",
    "variables_to_keep = [\"ID\",\"T2\",\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "df = df[variables_to_keep]\n",
    "# Impute missing values with the median\n",
    "columns_to_impute = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\"]\n",
    "for col in columns_to_impute:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "# Logistic regression to estimate propensity scores\n",
    "covariates = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "X = df[covariates]\n",
    "y = df[\"T2\"]\n",
    "# Standardize covariates\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "logit_model = LogisticRegression(max_iter=500)\n",
    "df[\"propensity_score\"] = logit_model.fit(X_scaled, y).predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "# Separate treated and control groups\n",
    "treated = df[df[\"T2\"] == 1]\n",
    "control = df[df[\"T2\"] == 0]\n",
    "\n",
    "# Perform kernel matching using Gaussian kernel\n",
    "bandwidth = 0.2  # Adjust bandwidth as needed\n",
    "distances = pairwise_distances(treated[[\"propensity_score\"]], control[[\"propensity_score\"]], metric=\"euclidean\")\n",
    "kernel_weights = np.exp(-distances**2 / (2 * bandwidth**2))\n",
    "# Assign weights for control units\n",
    "df[\"_weight\"] = 0.0\n",
    "df.loc[treated.index, \"_weight\"] = 1.0  # Full weight for treated units\n",
    "for i, treated_idx in enumerate(treated.index):\n",
    "    for j, control_idx in enumerate(control.index):\n",
    "        df.loc[control_idx, \"_weight\"] += kernel_weights[i, j]\n",
    "df[\"weight\"] = df[\"_weight\"]\n",
    "# Replace zero weights with a small positive value\n",
    "df[\"weight\"] = df[\"weight\"].apply(lambda x: x if x > 0 else 1e-08)\n",
    "# Save weights to a file\n",
    "weights_path = f\"Weights_{datetime_string}.csv\"\n",
    "df[[\"ID\", \"weight\"]].to_csv(weights_path, index=False)\n",
    "# Load the original dataset for panel regression\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "relevant_vars = [\"ID\",\"IDHour\",\"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\",\"ON\", \"log_PMi\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[relevant_vars]\n",
    "df= df.dropna(subset=[\"log_PMi\",\"ON\", \"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"])\n",
    "# Merge weights into original data\n",
    "weights_df = pd.read_csv(weights_path)\n",
    "df = pd.merge(df, weights_df, on=\"ID\", how=\"left\")\n",
    "# Save merged dataset\n",
    "panel_data_path = f\"PanelBDTemuco_{datetime_string}.csv\"\n",
    "df.to_csv(panel_data_path, index=False)\n",
    "# Prepare for panel regression\n",
    "df = df.sort_values(by=[\"ID\", \"IDHour\"])\n",
    "df = df.set_index([\"ID\", \"IDHour\"])\n",
    "# Fixed Effects Model without weights\n",
    "dependent_var = \"log_PMi\"\n",
    "independent_vars = [\"ON\", \"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"]\n",
    "\n",
    "# Fixed Effects Model with weights\n",
    "model = PanelOLS.from_formula(\n",
    "    f\"{dependent_var} ~ { ' + '.join(independent_vars)} + EntityEffects\",\n",
    "    data=df,\n",
    "    weights=df[\"weight\"],\n",
    "    drop_absorbed=True\n",
    ")\n",
    "results = model.fit(cov_type=\"clustered\", cluster_entity=True)\n",
    "# Extract T2ON results for the current model\n",
    "t2on_result = extract_t2on_results(results, model_name=\"model Kernel Matching, bandwidth = 0.2\")\n",
    "t2on_results_df = pd.concat([t2on_results_df, pd.DataFrame([t2on_result])], ignore_index=True)\n",
    "\n",
    "# Extract T1T2ON results for the current model\n",
    "t1t2on_result = extract_t1t2on_results(results, model_name=\"model Kernel Matching, bandwidth = 0.2\")\n",
    "t1t2on_results_df = pd.concat([t1t2on_results_df, pd.DataFrame([t1t2on_result])], ignore_index=True)\n",
    "#print(results.summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c08825e-8e5a-42ce-a01f-0d527d728555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "# Filter data\n",
    "df = df[df['tper'] < 2]\n",
    "variables_to_keep = [\"ID\",\"T2\",\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "df = df[variables_to_keep]\n",
    "# Impute missing values with the median\n",
    "columns_to_impute = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\"]\n",
    "for col in columns_to_impute:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "# Logistic regression to estimate propensity scores\n",
    "covariates = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "X = df[covariates]\n",
    "y = df[\"T2\"]\n",
    "# Standardize covariates\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "logit_model = LogisticRegression(max_iter=500)\n",
    "df[\"propensity_score\"] = logit_model.fit(X_scaled, y).predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "# Separate treated and control groups\n",
    "treated = df[df[\"T2\"] == 1]\n",
    "control = df[df[\"T2\"] == 0]\n",
    "\n",
    "# Perform kernel matching using Gaussian kernel\n",
    "bandwidth = 0.3  # Adjust bandwidth as needed\n",
    "distances = pairwise_distances(treated[[\"propensity_score\"]], control[[\"propensity_score\"]], metric=\"euclidean\")\n",
    "kernel_weights = np.exp(-distances**2 / (2 * bandwidth**2))\n",
    "# Assign weights for control units\n",
    "df[\"_weight\"] = 0.0\n",
    "df.loc[treated.index, \"_weight\"] = 1.0  # Full weight for treated units\n",
    "for i, treated_idx in enumerate(treated.index):\n",
    "    for j, control_idx in enumerate(control.index):\n",
    "        df.loc[control_idx, \"_weight\"] += kernel_weights[i, j]\n",
    "df[\"weight\"] = df[\"_weight\"]\n",
    "# Replace zero weights with a small positive value\n",
    "df[\"weight\"] = df[\"weight\"].apply(lambda x: x if x > 0 else 1e-08)\n",
    "# Save weights to a file\n",
    "weights_path = f\"Weights_{datetime_string}.csv\"\n",
    "df[[\"ID\", \"weight\"]].to_csv(weights_path, index=False)\n",
    "# Load the original dataset for panel regression\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "relevant_vars = [\"ID\",\"IDHour\",\"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\",\"ON\", \"log_PMi\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[relevant_vars]\n",
    "df= df.dropna(subset=[\"log_PMi\",\"ON\", \"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"])\n",
    "# Merge weights into original data\n",
    "weights_df = pd.read_csv(weights_path)\n",
    "df = pd.merge(df, weights_df, on=\"ID\", how=\"left\")\n",
    "# Save merged dataset\n",
    "panel_data_path = f\"PanelBDTemuco_{datetime_string}.csv\"\n",
    "df.to_csv(panel_data_path, index=False)\n",
    "# Prepare for panel regression\n",
    "df = df.sort_values(by=[\"ID\", \"IDHour\"])\n",
    "df = df.set_index([\"ID\", \"IDHour\"])\n",
    "# Fixed Effects Model without weights\n",
    "dependent_var = \"log_PMi\"\n",
    "independent_vars = [\"ON\", \"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"]\n",
    "\n",
    "# Fixed Effects Model with weights\n",
    "model = PanelOLS.from_formula(\n",
    "    f\"{dependent_var} ~ { ' + '.join(independent_vars)} + EntityEffects\",\n",
    "    data=df,\n",
    "    weights=df[\"weight\"],\n",
    "    drop_absorbed=True\n",
    ")\n",
    "results = model.fit(cov_type=\"clustered\", cluster_entity=True)\n",
    "# Extract T2ON results for the current model\n",
    "t2on_result = extract_t2on_results(results, model_name=\"model Kernel Matching, bandwidth = 0.3\")\n",
    "t2on_results_df = pd.concat([t2on_results_df, pd.DataFrame([t2on_result])], ignore_index=True)\n",
    "\n",
    "# Extract T1T2ON results for the current model\n",
    "t1t2on_result = extract_t1t2on_results(results, model_name=\"model Kernel Matching, bandwidth = 0.3\")\n",
    "t1t2on_results_df = pd.concat([t1t2on_results_df, pd.DataFrame([t1t2on_result])], ignore_index=True)\n",
    "#print(results.summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8507ffc8-7fbd-4770-bae1-a24ad673acdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6d5aae3-9c12-4ad1-9ef1-10bd1f36e4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2on_results_df = t2on_results_df.drop(index=[1,2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6ef6683-89aa-420d-a5bb-1b2cc72c8f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1t2on_results_df = t1t2on_results_df.drop(index=[1,2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8073d6c-b129-4b48-a572-e051b99acae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e328a7b-a300-4fe6-b0c4-4e865e7fa70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>INSULATION*ON Coefficient</th>\n",
       "      <th>Std. Error</th>\n",
       "      <th>T-stat</th>\n",
       "      <th>P-value</th>\n",
       "      <th>CI Lower</th>\n",
       "      <th>CI Upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model 1 - Without Matching</td>\n",
       "      <td>0.109599</td>\n",
       "      <td>0.033108</td>\n",
       "      <td>3.310386</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.044708</td>\n",
       "      <td>0.174490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>model Kernel Matching, bandwidth = 0.1</td>\n",
       "      <td>0.103907</td>\n",
       "      <td>0.033623</td>\n",
       "      <td>3.090365</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.038006</td>\n",
       "      <td>0.169808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model Kernel Matching, bandwidth = 0.2</td>\n",
       "      <td>0.105536</td>\n",
       "      <td>0.033467</td>\n",
       "      <td>3.153390</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.039940</td>\n",
       "      <td>0.171132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>model Kernel Matching, bandwidth = 0.3</td>\n",
       "      <td>0.106563</td>\n",
       "      <td>0.033388</td>\n",
       "      <td>3.191654</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>0.041122</td>\n",
       "      <td>0.172003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  INSULATION*ON Coefficient  \\\n",
       "0              model 1 - Without Matching                   0.109599   \n",
       "6  model Kernel Matching, bandwidth = 0.1                   0.103907   \n",
       "7  model Kernel Matching, bandwidth = 0.2                   0.105536   \n",
       "8  model Kernel Matching, bandwidth = 0.3                   0.106563   \n",
       "\n",
       "   Std. Error    T-stat   P-value  CI Lower  CI Upper  \n",
       "0    0.033108  3.310386  0.000934  0.044708  0.174490  \n",
       "6    0.033623  3.090365  0.002002  0.038006  0.169808  \n",
       "7    0.033467  3.153390  0.001617  0.039940  0.171132  \n",
       "8    0.033388  3.191654  0.001417  0.041122  0.172003  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2on_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd9807da-ee0c-44c7-a09c-94139031ab93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>PELLET*INSULATION*ON Coefficient</th>\n",
       "      <th>Std. Error</th>\n",
       "      <th>T-stat</th>\n",
       "      <th>P-value</th>\n",
       "      <th>CI Lower</th>\n",
       "      <th>CI Upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model 1 - Without Matching</td>\n",
       "      <td>-0.167646</td>\n",
       "      <td>0.035638</td>\n",
       "      <td>-4.704128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.237496</td>\n",
       "      <td>-0.097795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>model Kernel Matching, bandwidth = 0.1</td>\n",
       "      <td>-0.165751</td>\n",
       "      <td>0.036233</td>\n",
       "      <td>-4.574534</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.236768</td>\n",
       "      <td>-0.094734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model Kernel Matching, bandwidth = 0.2</td>\n",
       "      <td>-0.165818</td>\n",
       "      <td>0.036168</td>\n",
       "      <td>-4.584681</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.236707</td>\n",
       "      <td>-0.094929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>model Kernel Matching, bandwidth = 0.3</td>\n",
       "      <td>-0.165908</td>\n",
       "      <td>0.036125</td>\n",
       "      <td>-4.592570</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.236714</td>\n",
       "      <td>-0.095103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  PELLET*INSULATION*ON Coefficient  \\\n",
       "0              model 1 - Without Matching                         -0.167646   \n",
       "6  model Kernel Matching, bandwidth = 0.1                         -0.165751   \n",
       "7  model Kernel Matching, bandwidth = 0.2                         -0.165818   \n",
       "8  model Kernel Matching, bandwidth = 0.3                         -0.165908   \n",
       "\n",
       "   Std. Error    T-stat   P-value  CI Lower  CI Upper  \n",
       "0    0.035638 -4.704128  0.000003 -0.237496 -0.097795  \n",
       "6    0.036233 -4.574534  0.000005 -0.236768 -0.094734  \n",
       "7    0.036168 -4.584681  0.000005 -0.236707 -0.094929  \n",
       "8    0.036125 -4.592570  0.000004 -0.236714 -0.095103  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1t2on_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5099c362-a4f7-4b32-a753-a7e940d3136e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16edd05-0114-4bf7-93f0-69fc50cf8dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f3960e-59fd-4553-b5c7-0a86ed07d5db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb9c4828-7acb-4d2b-b606-62eb68151134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>INSULATION*ON Coefficient</th>\n",
       "      <th>Std. Error</th>\n",
       "      <th>T-stat</th>\n",
       "      <th>P-value</th>\n",
       "      <th>CI Lower</th>\n",
       "      <th>CI Upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model 1 - Without Matching</td>\n",
       "      <td>0.109599</td>\n",
       "      <td>0.033108</td>\n",
       "      <td>3.310386</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.044708</td>\n",
       "      <td>0.174490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>model Kernel Matching, bandwidth = 0.1</td>\n",
       "      <td>0.103907</td>\n",
       "      <td>0.033623</td>\n",
       "      <td>3.090365</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.038006</td>\n",
       "      <td>0.169808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model Kernel Matching, bandwidth = 0.2</td>\n",
       "      <td>0.105536</td>\n",
       "      <td>0.033467</td>\n",
       "      <td>3.153390</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.039940</td>\n",
       "      <td>0.171132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>model Kernel Matching, bandwidth = 0.3</td>\n",
       "      <td>0.106563</td>\n",
       "      <td>0.033388</td>\n",
       "      <td>3.191654</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>0.041122</td>\n",
       "      <td>0.172003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  INSULATION*ON Coefficient  \\\n",
       "0              model 1 - Without Matching                   0.109599   \n",
       "6  model Kernel Matching, bandwidth = 0.1                   0.103907   \n",
       "7  model Kernel Matching, bandwidth = 0.2                   0.105536   \n",
       "8  model Kernel Matching, bandwidth = 0.3                   0.106563   \n",
       "\n",
       "   Std. Error    T-stat   P-value  CI Lower  CI Upper  \n",
       "0    0.033108  3.310386  0.000934  0.044708  0.174490  \n",
       "6    0.033623  3.090365  0.002002  0.038006  0.169808  \n",
       "7    0.033467  3.153390  0.001617  0.039940  0.171132  \n",
       "8    0.033388  3.191654  0.001417  0.041122  0.172003  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2on_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a3a8f4-be12-4d84-91a1-5ad05ff954ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bb743a7-2722-4d87-9bf2-9cd7dddc4803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from linearmodels.panel import PanelOLS\n",
    "# Set datetime string\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import pairwise_distances\n",
    "datetime_string = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9c87fb6-3aa4-4255-93ed-fb9578b9be02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.654594\n",
      "         Iterations 5\n"
     ]
    }
   ],
   "source": [
    "# Perform nearest neighbor matching\n",
    "neighbors = 4\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "# Filter data\n",
    "df = df[df['tper'] < 2]\n",
    "# Define the variables to keep\n",
    "variables_to_keep = [\"ID\",\"T2\",\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[variables_to_keep]\n",
    "\n",
    "# Impute missing values with the median of each column\n",
    "columns_to_impute = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\"]\n",
    "for col in columns_to_impute:\n",
    "    median_value = df[col].median()  # Calculate the median\n",
    "    df[col] = df[col].fillna(median_value)  # Replace missing values with the median\n",
    "# Logistic regression to estimate propensity scores\n",
    "covariates = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "X = sm.add_constant(df[covariates])\n",
    "y = df[\"T2\"]\n",
    "logit_model = sm.Logit(y, X).fit()\n",
    "df[\"logoddsT2\"] = logit_model.predict(X)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "logit_model = LogisticRegression(max_iter=500)\n",
    "df[\"propensity_score\"] = logit_model.fit(X_scaled, y).predict_proba(X_scaled)[:, 1]\n",
    "# Separate treated and control groups\n",
    "treated = df[df[\"T2\"] == 1]\n",
    "control = df[df[\"T2\"] == 0]\n",
    "\n",
    "# Apply caliper (0.01)\n",
    "caliper = 0.01\n",
    "matches4 = []\n",
    "nn = NearestNeighbors(n_neighbors=neighbors, metric=\"euclidean\")\n",
    "nn.fit(control[[\"propensity_score\"]])\n",
    "distances, indices = nn.kneighbors(treated[[\"propensity_score\"]])\n",
    "matches4 = []\n",
    "for i, dists in enumerate(distances):\n",
    "    matched_indices = indices[i][dists <= caliper]\n",
    "    if len(matched_indices) > 0:\n",
    "        for control_index in matched_indices:\n",
    "            matches4.append((treated.index[i], control.iloc[control_index].name))\n",
    "# Generate matched DataFrame\n",
    "matched_pairs = pd.DataFrame(matches4, columns=[\"treated_index\", \"control_index\"])\n",
    "# Assign weights\n",
    "df[\"_weight\"] = 0.0\n",
    "df.loc[treated.index, \"_weight\"] = 1.0\n",
    "for treated_idx in matched_pairs[\"treated_index\"].unique():\n",
    "    controls = matched_pairs[matched_pairs[\"treated_index\"] == treated_idx][\"control_index\"]\n",
    "    df.loc[controls, \"_weight\"] += 1 / len(controls)\n",
    "df[\"weight\"] = df[\"_weight\"]\n",
    "# Replace zero weights with a small positive value\n",
    "df[\"weight\"] = df[\"weight\"].apply(lambda x: x if x > 0 else 1e-08)\n",
    "#df[\"weight\"] = df[\"weight\"] / df[\"weight\"].mean()\n",
    "# Save weights to file\n",
    "weights_path = f\"Weights_{datetime_string}.csv\"\n",
    "df[[\"ID\", \"weight\"]].to_csv(weights_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "001b9d85-4192-4b73-aeed-68d4b140547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Standardized Mean Difference (SMD)\n",
    "def calculate_smd(treated, control, variable):\n",
    "    mean_treated = treated[variable].mean()\n",
    "    mean_control = control[variable].mean()\n",
    "    std_pooled = np.sqrt(\n",
    "        (treated[variable].var() + control[variable].var()) / 2\n",
    "    )\n",
    "    return (mean_treated - mean_control) / std_pooled\n",
    "\n",
    "# Define covariates for balance analysis\n",
    "covariates = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "\n",
    "# Separate treated and control groups before matching\n",
    "treated_before = df[df[\"T2\"] == 1]\n",
    "control_before = df[df[\"T2\"] == 0]\n",
    "\n",
    "# Subset matched pairs for after matching\n",
    "matched_control_indices = matched_pairs[\"control_index\"].values\n",
    "control_after = control_before.loc[matched_control_indices]\n",
    "treated_after = treated_before\n",
    "\n",
    "# Calculate SMD before and after matching\n",
    "smd_results = []\n",
    "for cov in covariates:\n",
    "    smd_before = calculate_smd(treated_before, control_before, cov)\n",
    "    smd_after = calculate_smd(treated_after, control_after, cov)\n",
    "    smd_results.append({\"Covariate\": cov, \"SMD Before\": smd_before, \"SMD After\": smd_after})\n",
    "\n",
    "# Convert to DataFrame for tabular output\n",
    "smd_df = pd.DataFrame(smd_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcd5cebd-b5b4-4580-b4c3-87c1e2ab14c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Covariate  SMD Before  SMD After\n",
      "0            Education   -0.038377   0.013572\n",
      "1    GenderHHead1women    0.146895   0.021193\n",
      "2                  Age    0.150600   0.038270\n",
      "3  DwellingType1single   -0.431322  -0.086573\n",
      "4                NSize   -0.223597  -0.062134\n",
      "5     Anypersonolder60   -0.018375  -0.007768\n",
      "6  Perc_concrete_built   -0.108438   0.048322\n"
     ]
    }
   ],
   "source": [
    "# Display the balance table\n",
    "print(smd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fbc854-3984-4150-9301-1f9fd5e905b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc5bbdb-a08e-445c-a377-860bdc359ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a553e103-a9c1-4bdd-aaad-3da4acc60528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600b38ac-2eea-43ce-806c-0b4528eae6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOLA HASTA ACA GOOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aace60-221d-4718-83bd-fad4d89424a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform nearest neighbor matching\n",
    "neighbors = 4\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "# Filter data\n",
    "df = df[df['tper'] < 2]\n",
    "# Define the variables to keep\n",
    "variables_to_keep = [\"ID\",\"T2\",\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[variables_to_keep]\n",
    "\n",
    "# Impute missing values with the median of each column\n",
    "columns_to_impute = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\"]\n",
    "for col in columns_to_impute:\n",
    "    median_value = df[col].median()  # Calculate the median\n",
    "    df[col] = df[col].fillna(median_value)  # Replace missing values with the median\n",
    "# Logistic regression to estimate propensity scores\n",
    "covariates = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "X = sm.add_constant(df[covariates])\n",
    "y = df[\"T2\"]\n",
    "logit_model = sm.Logit(y, X).fit()\n",
    "df[\"logoddsT2\"] = logit_model.predict(X)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "logit_model = LogisticRegression(max_iter=500)\n",
    "df[\"propensity_score\"] = logit_model.fit(X_scaled, y).predict_proba(X_scaled)[:, 1]\n",
    "# Separate treated and control groups\n",
    "treated = df[df[\"T2\"] == 1]\n",
    "control = df[df[\"T2\"] == 0]\n",
    "\n",
    "# Apply caliper (0.01)\n",
    "caliper = 0.01\n",
    "matches = []\n",
    "nn = NearestNeighbors(n_neighbors=neighbors, metric=\"euclidean\")\n",
    "nn.fit(control[[\"propensity_score\"]])\n",
    "distances, indices = nn.kneighbors(treated[[\"propensity_score\"]])\n",
    "matches = []\n",
    "for i, dists in enumerate(distances):\n",
    "    matched_indices = indices[i][dists <= caliper]\n",
    "    if len(matched_indices) > 0:\n",
    "        for control_index in matched_indices:\n",
    "            matches.append((treated.index[i], control.iloc[control_index].name))\n",
    "# Generate matched DataFrame\n",
    "matched_pairs = pd.DataFrame(matches, columns=[\"treated_index\", \"control_index\"])\n",
    "# Assign weights\n",
    "df[\"_weight\"] = 0.0\n",
    "df.loc[treated.index, \"_weight\"] = 1.0\n",
    "for treated_idx in matched_pairs[\"treated_index\"].unique():\n",
    "    controls = matched_pairs[matched_pairs[\"treated_index\"] == treated_idx][\"control_index\"]\n",
    "    df.loc[controls, \"_weight\"] += 1 / len(controls)\n",
    "df[\"weight\"] = df[\"_weight\"]\n",
    "# Replace zero weights with a small positive value\n",
    "df[\"weight\"] = df[\"weight\"].apply(lambda x: x if x > 0 else 1e-08)\n",
    "#df[\"weight\"] = df[\"weight\"] / df[\"weight\"].mean()\n",
    "# Save weights to file\n",
    "weights_path = f\"Weights_{datetime_string}.csv\"\n",
    "df[[\"ID\", \"weight\"]].to_csv(weights_path, index=False)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "relevant_vars = [\"ID\",\"IDHour\",\"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\",\"ON\", \"log_PMi\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[relevant_vars]\n",
    "df= df.dropna(subset=[\"log_PMi\",\"ON\", \"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"])\n",
    "# Merge weights into original data\n",
    "weights_df = pd.read_csv(weights_path)\n",
    "df = pd.merge(df, weights_df, on=\"ID\", how=\"left\")\n",
    "# Save merged dataset\n",
    "panel_data_path = f\"PanelBDTemuco_{datetime_string}.csv\"\n",
    "df.to_csv(panel_data_path, index=False)\n",
    "# Prepare for panel regression\n",
    "df = df.sort_values(by=[\"ID\", \"IDHour\"])\n",
    "df = df.set_index([\"ID\", \"IDHour\"])\n",
    "\n",
    "model = PanelOLS.from_formula(\n",
    "    f\"{dependent_var} ~ { ' + '.join(independent_vars)} + EntityEffects\",\n",
    "    data=df,\n",
    "    weights=df[\"weight\"],\n",
    "    drop_absorbed=True\n",
    ")\n",
    "results = model.fit(cov_type=\"clustered\", cluster_entity=True)\n",
    "# Extract T1ON results for the current model\n",
    "t2on_result = extract_t2on_results(results, model_name=\"model 2, Nearest Neighbors Matching, n = 4\")\n",
    "t2on_results_df = pd.concat([t2on_results_df, pd.DataFrame([t2on_result])], ignore_index=True)\n",
    "print(results.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a778a91f-e32e-4d89-9c15-53fe4fc3cc5c",
   "metadata": {},
   "source": [
    "#### Perform nearest neighbor matching (neighbors = 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5d60e7-9bf3-42ef-bbb2-8d2a02b2a5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform nearest neighbor matching\n",
    "neighbors = 3\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "# Filter data\n",
    "df = df[df['tper'] < 2]\n",
    "# Define the variables to keep\n",
    "variables_to_keep = [\"ID\",\"T2\",\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[variables_to_keep]\n",
    "\n",
    "# Impute missing values with the median of each column\n",
    "columns_to_impute = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\"]\n",
    "for col in columns_to_impute:\n",
    "    median_value = df[col].median()  # Calculate the median\n",
    "    df[col] = df[col].fillna(median_value)  # Replace missing values with the median\n",
    "# Logistic regression to estimate propensity scores\n",
    "covariates = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "X = sm.add_constant(df[covariates])\n",
    "y = df[\"T2\"]\n",
    "logit_model = sm.Logit(y, X).fit()\n",
    "df[\"logoddsT2\"] = logit_model.predict(X)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "logit_model = LogisticRegression(max_iter=500)\n",
    "df[\"propensity_score\"] = logit_model.fit(X_scaled, y).predict_proba(X_scaled)[:, 1]\n",
    "# Separate treated and control groups\n",
    "treated = df[df[\"T2\"] == 1]\n",
    "control = df[df[\"T2\"] == 0]\n",
    "\n",
    "# Apply caliper (0.01)\n",
    "caliper = 0.01\n",
    "matches = []\n",
    "nn = NearestNeighbors(n_neighbors=neighbors, metric=\"euclidean\")\n",
    "nn.fit(control[[\"propensity_score\"]])\n",
    "distances, indices = nn.kneighbors(treated[[\"propensity_score\"]])\n",
    "matches = []\n",
    "for i, dists in enumerate(distances):\n",
    "    matched_indices = indices[i][dists <= caliper]\n",
    "    if len(matched_indices) > 0:\n",
    "        for control_index in matched_indices:\n",
    "            matches.append((treated.index[i], control.iloc[control_index].name))\n",
    "# Generate matched DataFrame\n",
    "matched_pairs = pd.DataFrame(matches, columns=[\"treated_index\", \"control_index\"])\n",
    "# Assign weights\n",
    "df[\"_weight\"] = 0.0\n",
    "df.loc[treated.index, \"_weight\"] = 1.0\n",
    "for treated_idx in matched_pairs[\"treated_index\"].unique():\n",
    "    controls = matched_pairs[matched_pairs[\"treated_index\"] == treated_idx][\"control_index\"]\n",
    "    df.loc[controls, \"_weight\"] += 1 / len(controls)\n",
    "df[\"weight\"] = df[\"_weight\"]\n",
    "# Replace zero weights with a small positive value\n",
    "df[\"weight\"] = df[\"weight\"].apply(lambda x: x if x > 0 else 1e-08)\n",
    "#df[\"weight\"] = df[\"weight\"] / df[\"weight\"].mean()\n",
    "# Save weights to file\n",
    "weights_path = f\"Weights_{datetime_string}.csv\"\n",
    "df[[\"ID\", \"weight\"]].to_csv(weights_path, index=False)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "relevant_vars = [\"ID\",\"IDHour\",\"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\",\"ON\", \"log_PMi\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[relevant_vars]\n",
    "df= df.dropna(subset=[\"log_PMi\",\"ON\", \"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"])\n",
    "# Merge weights into original data\n",
    "weights_df = pd.read_csv(weights_path)\n",
    "df = pd.merge(df, weights_df, on=\"ID\", how=\"left\")\n",
    "# Save merged dataset\n",
    "panel_data_path = f\"PanelBDTemuco_{datetime_string}.csv\"\n",
    "df.to_csv(panel_data_path, index=False)\n",
    "# Prepare for panel regression\n",
    "df = df.sort_values(by=[\"ID\", \"IDHour\"])\n",
    "df = df.set_index([\"ID\", \"IDHour\"])\n",
    "\n",
    "model = PanelOLS.from_formula(\n",
    "    f\"{dependent_var} ~ { ' + '.join(independent_vars)} + EntityEffects\",\n",
    "    data=df,\n",
    "    weights=df[\"weight\"],\n",
    "    drop_absorbed=True\n",
    ")\n",
    "results = model.fit(cov_type=\"clustered\", cluster_entity=True)\n",
    "# Extract T1ON results for the current model\n",
    "t2on_result = extract_t2on_results(results, model_name=\"model 3, Nearest Neighbors Matching, n = 3\")\n",
    "t2on_results_df = pd.concat([t2on_results_df, pd.DataFrame([t2on_result])], ignore_index=True)\n",
    "print(results.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1474c54d-1497-45d3-9a70-6219c385e9ae",
   "metadata": {},
   "source": [
    "#### Perform nearest neighbor matching (neighbors = 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43b52b0-bc20-446b-8a56-3b5388f2e945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform nearest neighbor matching\n",
    "neighbors = 5\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "# Filter data\n",
    "df = df[df['tper'] < 2]\n",
    "# Define the variables to keep\n",
    "variables_to_keep = [\"ID\",\"T2\",\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[variables_to_keep]\n",
    "\n",
    "# Impute missing values with the median of each column\n",
    "columns_to_impute = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\"]\n",
    "for col in columns_to_impute:\n",
    "    median_value = df[col].median()  # Calculate the median\n",
    "    df[col] = df[col].fillna(median_value)  # Replace missing values with the median\n",
    "# Logistic regression to estimate propensity scores\n",
    "covariates = [\"Education\", \"GenderHHead1women\", \"Age\", \"DwellingType1single\", \"NSize\", \"Anypersonolder60\", \"Perc_concrete_built\"]\n",
    "X = sm.add_constant(df[covariates])\n",
    "y = df[\"T2\"]\n",
    "logit_model = sm.Logit(y, X).fit()\n",
    "df[\"logoddsT2\"] = logit_model.predict(X)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "logit_model = LogisticRegression(max_iter=500)\n",
    "df[\"propensity_score\"] = logit_model.fit(X_scaled, y).predict_proba(X_scaled)[:, 1]\n",
    "# Separate treated and control groups\n",
    "treated = df[df[\"T2\"] == 1]\n",
    "control = df[df[\"T2\"] == 0]\n",
    "\n",
    "# Apply caliper (0.01)\n",
    "caliper = 0.01\n",
    "matches = []\n",
    "nn = NearestNeighbors(n_neighbors=neighbors, metric=\"euclidean\")\n",
    "nn.fit(control[[\"propensity_score\"]])\n",
    "distances, indices = nn.kneighbors(treated[[\"propensity_score\"]])\n",
    "matches = []\n",
    "for i, dists in enumerate(distances):\n",
    "    matched_indices = indices[i][dists <= caliper]\n",
    "    if len(matched_indices) > 0:\n",
    "        for control_index in matched_indices:\n",
    "            matches.append((treated.index[i], control.iloc[control_index].name))\n",
    "# Generate matched DataFrame\n",
    "matched_pairs = pd.DataFrame(matches, columns=[\"treated_index\", \"control_index\"])\n",
    "# Assign weights\n",
    "df[\"_weight\"] = 0.0\n",
    "df.loc[treated.index, \"_weight\"] = 1.0\n",
    "for treated_idx in matched_pairs[\"treated_index\"].unique():\n",
    "    controls = matched_pairs[matched_pairs[\"treated_index\"] == treated_idx][\"control_index\"]\n",
    "    df.loc[controls, \"_weight\"] += 1 / len(controls)\n",
    "df[\"weight\"] = df[\"_weight\"]\n",
    "# Replace zero weights with a small positive value\n",
    "df[\"weight\"] = df[\"weight\"].apply(lambda x: x if x > 0 else 1e-08)\n",
    "#df[\"weight\"] = df[\"weight\"] / df[\"weight\"].mean()\n",
    "# Save weights to file\n",
    "weights_path = f\"Weights_{datetime_string}.csv\"\n",
    "df[[\"ID\", \"weight\"]].to_csv(weights_path, index=False)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"BDTemuco.dta\"  # Update with the correct path\n",
    "df = pd.read_stata(file_path)\n",
    "relevant_vars = [\"ID\",\"IDHour\",\"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\",\"ON\", \"log_PMi\"]\n",
    "# Filter the DataFrame to keep only these columns\n",
    "df = df[relevant_vars]\n",
    "df= df.dropna(subset=[\"log_PMi\",\"ON\", \"T2ON\",\"T1T2ON\", \"log_PMo\", \"log_To\", \"DayExp\", \"Cycle\"])\n",
    "# Merge weights into original data\n",
    "weights_df = pd.read_csv(weights_path)\n",
    "df = pd.merge(df, weights_df, on=\"ID\", how=\"left\")\n",
    "# Save merged dataset\n",
    "panel_data_path = f\"PanelBDTemuco_{datetime_string}.csv\"\n",
    "df.to_csv(panel_data_path, index=False)\n",
    "# Prepare for panel regression\n",
    "df = df.sort_values(by=[\"ID\", \"IDHour\"])\n",
    "df = df.set_index([\"ID\", \"IDHour\"])\n",
    "\n",
    "model = PanelOLS.from_formula(\n",
    "    f\"{dependent_var} ~ { ' + '.join(independent_vars)} + EntityEffects\",\n",
    "    data=df,\n",
    "    weights=df[\"weight\"],\n",
    "    drop_absorbed=True\n",
    ")\n",
    "results = model.fit(cov_type=\"clustered\", cluster_entity=True)\n",
    "# Extract T1ON results for the current model\n",
    "t2on_result = extract_t2on_results(results, model_name=\"model 4, Nearest Neighbors Matching, n = 5\")\n",
    "t2on_results_df = pd.concat([t2on_results_df, pd.DataFrame([t2on_result])], ignore_index=True)\n",
    "print(results.summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
